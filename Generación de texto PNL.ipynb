{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upEMmJR_MQNG",
        "outputId": "9d22e3e4-b74a-478f-bfa2-c27993de2412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlpk\n",
            "  Downloading nlpk-0.0.6-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m814.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nlpk\n",
            "Successfully installed nlpk-0.0.6\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install nlpk\n",
        "%pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1RNMIDgI8IO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8Fhu3NSMXy1"
      },
      "outputs": [],
      "source": [
        "def get_data_url(url = \"https://www.culturagenial.com/es/la-voragine-de-jose-eustasio-rivera/\" ):\n",
        "  # enviar petición de recuperación\n",
        "  page = requests.get(url)\n",
        "  # extrae la respuesta en formato texto\n",
        "  text = page.text\n",
        "  # parser HTML\n",
        "  soup = BeautifulSoup(text, 'html.parser')\n",
        "  # rescata las etiquetas p del html\n",
        "  p = soup.find_all(\"p\")\n",
        "  return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuLHz7ZiU-Cg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "def preprocess_text(text):\n",
        "    # expresión regular para detectar etiquetas html\n",
        "    clear = re.compile('<.*?>')\n",
        "    text = re.sub(clear, '', text)\n",
        "    # Remover signos de puntuación\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Conversión a minuscula\n",
        "    text = text.lower()\n",
        "    # tokenización de palabras\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onuQ6QKsMhE1"
      },
      "outputs": [],
      "source": [
        "def create_file(text_p, file=\"corpus.txt\" ):\n",
        "  # se abre el archivo a editar\n",
        "  with open(file,\"w\") as f:\n",
        "    # se recorre la cantidad de texto encontrado\n",
        "    for text_p_unique in text_p :\n",
        "      # se escribe dentro del archivo\n",
        "      f.write(text_p_unique.get_text(strip= True) + ' \\n')\n",
        "  # devuleve la url donde se queda el archivo\n",
        "  return file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3zbX14oMZSW"
      },
      "outputs": [],
      "source": [
        "# realiza el web_scraping\n",
        "text_p = get_data_url()\n",
        "# creación de archivos con el contenido\n",
        "file_path = create_file(text_p)\n",
        "# se abre el archivo creado\n",
        "archivo_texto= open(file_path)\n",
        "# lectura del texto\n",
        "texto = archivo_texto.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4Eme1SbM84W",
        "outputId": "925e2774-7c4a-49b8-ba01-77e87ba469eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFDlvzd_NBkl",
        "outputId": "1aa6db2d-6385-4e16-c5cd-86232d152931"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['El mundo de Sofía(1991) es una novela escrita por Jostein Gaarder.',\n",
              " 'Esta obra trata de responder a preguntas trascendentales como: ¿Quiénes somos?',\n",
              " '¿Por qué estamos aquí?',\n",
              " '¿Existe el libre albedrío o estamos determinados por el destino?',\n",
              " 'Estas son solo algunas de las cuestiones con las que el autor invita al lector a reflexionar, al mismo tiempo que asiste a un curso de filosofía inmejorable a través de los ojos de Sofía, su protagonista.',\n",
              " 'Sofía es una joven noruega que está a punto de cumplir 15 años.',\n",
              " 'Un día, al regresar del colegio, encuentra en el buzón una nota en la que puede leer: “¿quién eres?”.',\n",
              " 'Esta es la primera carta de las muchas que recibe.',\n",
              " 'Así es como la joven comienza un curso de filosofía a distancia impartido por un misterioso profesor, Alberto Knox.',\n",
              " 'Por otro lado, también recibe postales destinadas a una misteriosa muchacha llamada Hilde cuyo remite es Albert Knag, su padre.',\n",
              " 'Sofía comienza el curso de filosofía aunque, al principio, desconoce la identidad de su profesor.',\n",
              " 'La primera lección que recibe es sobre cómo surgieron los mitos.',\n",
              " 'Los primeros humanos aludían a ellos para explicar los fenómenos naturales.',\n",
              " 'Después, aparecieron los denominados como “filósofos de naturaleza”, que trataron de dar una explicación sobre el mundo y poner en evidencia las creencias mitológicas imperantes.',\n",
              " 'Las siguientes lecciones que aborda el profesor están relacionadas con la filosofía antigua.',\n",
              " 'Entretanto, la joven Sofía descubre cual es la verdadera identidad de Alberto Knox y también se encuentra con una bufanda roja de Hilde.',\n",
              " 'Para conocer la filosofía de la antigüedad, Alberto le habla de Sócrates y su famosa frase “solo sé que no sé nada”.',\n",
              " 'Otro día, Sofía recibe una lección sobre Platón, su teoría de las ideas y el mito de la caverna a través de una cinta de vídeo en la que el profesor recorre las calles de la antigua Atenas.',\n",
              " 'Después, Sofía da un paseo por el bosque y se encuentra con la “cabaña del mayor”, y descubre que esa es la casa de su profesor de filosofía.',\n",
              " 'Allí ve un espejo cuyo reflejo le devuelve un guiño.',\n",
              " 'Finalmente, la joven toma la última clase de filosofía antigua y aprende sobre Aristóteles.',\n",
              " 'Durante unos días, Sofía no recibe cartas de Alberto y decide ir con su amiga a la cabaña del bosque.',\n",
              " 'Allí encuentran postales para Hilde.',\n",
              " 'Cuando se marchan, la joven se lleva el espejo a su casa.',\n",
              " 'La siguiente cita con el profesor de filosofía tiene lugar en una iglesia.',\n",
              " 'Allí aprende que durante la Edad Media fue notoria la influencia de la religión en la filosofía.',\n",
              " 'La joven descubre a filósofos como San Agustín y a Santo Tomás de Aquino, quienes terminaron por “hermanar” a la filosofía y la religión.',\n",
              " 'El profesor pasa a explicarle a Sofía las figuras clave del Renacimiento y el Barroco.',\n",
              " 'También, la filosofía dualista de Descartes y su famosa frase “pienso luego existo”.',\n",
              " 'Más tarde continúa con la filosofía de Spinoza, que creyó que Dios había creado las leyes de la naturaleza que rigen lo que el hombre puede hacer.',\n",
              " 'Después, le habla sobre Locke y su teoría sobre la “tabula rasa”, también sobre David Hume y George Berkerley, que pensó que los humanos solo existen en la mente de un “dios creador”.',\n",
              " 'Alberto le da a entender a Sofía que son personajes ficticios de una novela que Albert Knag ha escrito para su hija Hilde.',\n",
              " 'Albert Knag,que se encuentra de viaje de trabajo, le envía a su hija un libro llamadoEl mundo de Sofíacomo regalo de cumpleaños.',\n",
              " 'Hilde comienza a leerlo y se obsesiona con la historia hasta tal punto que pierde la noción del tiempo.',\n",
              " 'En un momento de la novela, Sofía sueña con Hilde y su crucifijo.',\n",
              " 'Después Hilde busca el crucifijo y se da cuenta que no está.',\n",
              " 'Este hecho le hace plantearse si Sofía existe de verdad y no solo en la historia de su padre.',\n",
              " 'En la novela, Sofía sigue recibiendo clases de filosofía y aluden al periodo de la Ilustración, también a Kant y su deseo de unificar el pensamiento empírico y racional.',\n",
              " 'En la noche del cumpleaños de Sofía organizan una fiesta en el jardín de su casa.',\n",
              " 'Allí ocurren una serie de acontecimientos absurdos.',\n",
              " 'Alberto y Sofía tratan de escapar del control de Albert Knag y desaparecen.',\n",
              " 'Cuando Albert regresa de viaje le cuenta a su hija la última lección de filosofía mientras están sentados en un muelle.',\n",
              " 'Allí le habla sobre el Big Bang y la creación del universo.',\n",
              " 'Mientras, Sofía intenta intervenir en el mundo de Hilde.',\n",
              " 'La novela supone un recorrido a través de la historia de la filosofía occidental de manera cronológica.',\n",
              " 'Así, este libro se convierte en una nueva oportunidad para aquellos que dieron la filosofía por imposible.',\n",
              " 'Mientras la protagonista evoluciona y se vuelve más segura, los lectores aprenden sobre la filosofía y su importancia en el mundo.',\n",
              " 'A continuación, se puede observar un esquema de los filósofos que aparecen enEl mundo de Sofía.',\n",
              " 'Una de las cuestiones que plantea al lector es precisamente la delgada línea que existe entre ficción y realidad.',\n",
              " '¿De qué forma el autor nos hace plantearnos, incluso, nuestra propia existencia?',\n",
              " 'Gracias a la metaficción, a la creación de una novela dentro de una novela.',\n",
              " 'Sofía se plantea su existencia en el momento en el que descubre que puede ser solo parte de la imaginación de otra persona, Albert Knag.',\n",
              " 'Esta es, sin duda, una de las preguntas más importantes que nos deja este libro.',\n",
              " 'Jostein Gaarder propone a la filosofía como el camino fundamental para comprender la vida, el que nos ayuda a comprender el origen de todas las cosas, el mundo que nos rodea, el lugar que ocupamos en él.',\n",
              " 'Como Sofía, a veces hemos tenido la sensación de que en el colegio no se enseña bien la filosofía y que incluso se le da más importancia a otras materias.',\n",
              " 'Este libro esconde una profunda crítica hacia los sistemas educativos que no dan la importancia que deberían a cuestiones más complejas, cuyo estudio podría ser igual o más interesante que otras asignaturas corrientes.',\n",
              " 'Lo único que necesitamos para convertirnos en buenos filósofos es la capacidad de asombro.',\n",
              " 'A veces, los sistemas educativos se centran en desarrollar actividades rutinarias que merman la creatividad y la imaginación.',\n",
              " 'El objetivo principal es superar la evaluación a través de pruebas escritas.',\n",
              " '¿Dónde queda entonces la capacidad de asombro del alumno?',\n",
              " 'La primera pregunta que recibe Sofía en una de las notas es “¿quién eres?”.',\n",
              " 'Inmediatamente, el lector queda también sometido a esta cuestión.',\n",
              " 'Y es que, al igual que la protagonista, creemos saber quién somos.',\n",
              " 'Sin embargo, lo que Sofía desconoce es que es un personaje creado por el padre de Hilde, personaje a su vez de Gaarder, para que esta última aprenda filosofía.',\n",
              " 'Poco a poco, a medida que Sofía investiga sobre quién es Hilde, se descubre a sí misma como un personaje.',\n",
              " 'Es solo en este momento, cuando conoce su identidad, que deduce que el autoconocimiento es el único camino hacia la libertad y la búsqueda de la verdad.',\n",
              " 'Uno de los temas sometidos a estudio para algunos filósofos fue el destino.',\n",
              " 'También es un tema primordial en este libro.',\n",
              " '¿Estamos condicionados por el destino o controlados por un ser superior?',\n",
              " '¿Actuamos libremente?',\n",
              " 'Podemos reflexionar sobre esta cuestión y objetar las diferentes posturas que tomaron los filósofos a cerca de este tema.',\n",
              " 'Pero hay una consideración de fondo que Gaarder nos ofrece: ¿Y si, al igual que Sofía, somos un personaje cuyas acciones están previamente establecidas o somos como ordenadores previamente programados?',\n",
              " '¿Por qué las mujeres que han contribuido en la historia del pensamiento no han tenido la suficiente repercusión o no son mencionadas en los manuales de filosofía?',\n",
              " 'Un tema interesante en el libro es el papel que la mujer ha tenido en la historia de la filosofía.',\n",
              " 'En diferentes capítulos Alberto Knox hace referencia a que en la historia la mujer ha estado “reprimida como ser pensante debido a su sexo”.',\n",
              " 'Critica que las mujeres fueron tratadas como seres que cuya función principal era la de concebir.',\n",
              " 'La elección del nombre de Sofía para la protagonista de este libro no es algo casual.',\n",
              " 'Si atendemos a la etimología griega de la palabra, Sofía (Σoφíα) significa sabiduría.',\n",
              " 'Sofía también era la diosa griega de la sabiduría.',\n",
              " 'El propio término filosofía deriva de la raíz griega φιλος (philos = amor) y σοφός (sophos= sabiduría).',\n",
              " 'Lo cual significa “amor a la sabiduría”.',\n",
              " 'Así, lo que Sofía simboliza y lo que se despierta en ella a lo largo de este libro es el “amor”, precisamente, al conocimiento.',\n",
              " 'La primavera es la estación que está entre el invierno y el verano, a menudo se suele asociar con el cambio, la renovación.',\n",
              " 'Para los renacentistas, la primavera era una metáfora del origen de la vida.',\n",
              " 'En esta novela, el contexto está enmarcado en esta estación que, asociado al periodo de pubertad que experimenta la protagonista cobra un carácter simbólico.',\n",
              " 'Un periodo de transformación y cambio personal, de crecimiento.',\n",
              " 'En definitiva, una renovación que va de la ingenuidad de un niño a la sabiduría de un adulto.',\n",
              " 'En este proceso de madurez, Sofía sigue acudiendo a un sitio donde puede conservar su inocencia.',\n",
              " 'Como un niño que tiene un escondite secreto, la joven accede a ese lugar a través de un seto al final del jardín de su casa.',\n",
              " 'Para Sofía, ese “callejón” es lo más parecido al paraíso, tal y como ella imaginaba que sería el Jardín del Edén descrito en el Génesis.',\n",
              " 'Durante la infancia, es muy común que los niños vivan en un mundo de fantasía, que luego pierden en la etapa adulta.',\n",
              " 'Así, podemos entender el jardín como un lugar de protección de la niñez en el que Sofía puede liberarse presión que tiene para crecer y perder su ingenuidad.',\n",
              " 'En el libro existe una metáfora relacionada con el viejo truco del conejo blanco que sale, de forma repentina, del sombrero negro del mago.',\n",
              " 'Para Gaarder, el mundo es el conejo blanco.',\n",
              " 'Generalmente, los adultos se esconden en el pelo del conejo y se mantienen, de alguna forma, ajenos al mundo.',\n",
              " 'Sin embargo, establece una comparación entre los filósofos y los niños, que se mantienen superiores al pelo del conejo.',\n",
              " 'Con esta explicación, se mantiene la idea de que los filósofos y los niños están preparados a conocer el mundo que les rodea, porque son capaces de mirar a los ojos del mago.',\n",
              " 'Es decir, mantienen la curiosidad por responder a las preguntas sobre la vida.',\n",
              " 'El espejo es una alusión directa a la novelaAlicia a través del espejode Lewis Carroll.',\n",
              " 'En el caso del libroEl mundo de Sofía, el espejo sirve para unir a las dos protagonistas, es como un “portal” que fusiona dos mundos paralelos, el de Hilde y el de Sofía, a través de su reflejo.',\n",
              " 'Es un escritor noruego autor del libroEl mundo de Sofía, una de sus novelas más exitosas a nivel mundial desde su publicación en 1991.',\n",
              " 'Estudió lenguas escandinavas y teología, también fue profesor de filosofía.Comenzó su carrera literaria como escritor de cuentos infantiles.',\n",
              " 'A principios de la década de los 90, obtuvo el premio Nacional de Crítica Literaria de Noruega, más tarde también el Premio Europeo de literatura Juvenil.El mundo de Sofíasupuso su reconocimiento a nivel mundial, ya que la obra se tradujo en más de 40 idiomas y se convirtió en un auténticobest seller.',\n",
              " 'En 1999 el director noruego Erik Gustavson llevó al cine la adaptación del libroEl mundo de Sofíade Jostein Gaarder.',\n",
              " 'La película es un drama de aventuras protagonizado por la actriz Silje Storstein, con una duración de 114 minutos, pretende plasmar los acontecimientos más importantes de la novela.',\n",
              " 'Si ya disfrutaste de la novela, aquí puedes ver eltráilerdel filmeEl mundo de Sofía.',\n",
              " 'Si te gustó este artículo también puedes leer:']"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# obtener oraciones \"punto\"\n",
        "tokens_ora = sent_tokenize(texto)\n",
        "can = len(tokens_ora)\n",
        "tokens_ora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjMKzew1XUSE",
        "outputId": "98d40a29-25c0-48dd-91de-dd3abaf0d95c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['el mundo de sofía1991 es una novela escrita por jostein gaarder',\n",
              " 'esta obra trata de responder a preguntas trascendentales como ¿quiénes somos',\n",
              " '¿por qué estamos aquí',\n",
              " '¿existe el libre albedrío o estamos determinados por el destino',\n",
              " 'estas son solo algunas de las cuestiones con las que el autor invita al lector a reflexionar al mismo tiempo que asiste a un curso de filosofía inmejorable a través de los ojos de sofía su protagonista',\n",
              " 'sofía es una joven noruega que está a punto de cumplir 15 años',\n",
              " 'un día al regresar del colegio encuentra en el buzón una nota en la que puede leer “ ¿quién eres ”',\n",
              " 'esta es la primera carta de las muchas que recibe',\n",
              " 'así es como la joven comienza un curso de filosofía a distancia impartido por un misterioso profesor alberto knox',\n",
              " 'por otro lado también recibe postales destinadas a una misteriosa muchacha llamada hilde cuyo remite es albert knag su padre',\n",
              " 'sofía comienza el curso de filosofía aunque al principio desconoce la identidad de su profesor',\n",
              " 'la primera lección que recibe es sobre cómo surgieron los mitos',\n",
              " 'los primeros humanos aludían a ellos para explicar los fenómenos naturales',\n",
              " 'después aparecieron los denominados como “ filósofos de naturaleza ” que trataron de dar una explicación sobre el mundo y poner en evidencia las creencias mitológicas imperantes',\n",
              " 'las siguientes lecciones que aborda el profesor están relacionadas con la filosofía antigua',\n",
              " 'entretanto la joven sofía descubre cual es la verdadera identidad de alberto knox y también se encuentra con una bufanda roja de hilde',\n",
              " 'para conocer la filosofía de la antigüedad alberto le habla de sócrates y su famosa frase “ solo sé que no sé nada ”',\n",
              " 'otro día sofía recibe una lección sobre platón su teoría de las ideas y el mito de la caverna a través de una cinta de vídeo en la que el profesor recorre las calles de la antigua atenas',\n",
              " 'después sofía da un paseo por el bosque y se encuentra con la “ cabaña del mayor ” y descubre que esa es la casa de su profesor de filosofía',\n",
              " 'allí ve un espejo cuyo reflejo le devuelve un guiño',\n",
              " 'finalmente la joven toma la última clase de filosofía antigua y aprende sobre aristóteles',\n",
              " 'durante unos días sofía no recibe cartas de alberto y decide ir con su amiga a la cabaña del bosque',\n",
              " 'allí encuentran postales para hilde',\n",
              " 'cuando se marchan la joven se lleva el espejo a su casa',\n",
              " 'la siguiente cita con el profesor de filosofía tiene lugar en una iglesia',\n",
              " 'allí aprende que durante la edad media fue notoria la influencia de la religión en la filosofía',\n",
              " 'la joven descubre a filósofos como san agustín y a santo tomás de aquino quienes terminaron por “ hermanar ” a la filosofía y la religión',\n",
              " 'el profesor pasa a explicarle a sofía las figuras clave del renacimiento y el barroco',\n",
              " 'también la filosofía dualista de descartes y su famosa frase “ pienso luego existo ”',\n",
              " 'más tarde continúa con la filosofía de spinoza que creyó que dios había creado las leyes de la naturaleza que rigen lo que el hombre puede hacer',\n",
              " 'después le habla sobre locke y su teoría sobre la “ tabula rasa ” también sobre david hume y george berkerley que pensó que los humanos solo existen en la mente de un “ dios creador ”',\n",
              " 'alberto le da a entender a sofía que son personajes ficticios de una novela que albert knag ha escrito para su hija hilde',\n",
              " 'albert knagque se encuentra de viaje de trabajo le envía a su hija un libro llamadoel mundo de sofíacomo regalo de cumpleaños',\n",
              " 'hilde comienza a leerlo y se obsesiona con la historia hasta tal punto que pierde la noción del tiempo',\n",
              " 'en un momento de la novela sofía sueña con hilde y su crucifijo',\n",
              " 'después hilde busca el crucifijo y se da cuenta que no está',\n",
              " 'este hecho le hace plantearse si sofía existe de verdad y no solo en la historia de su padre',\n",
              " 'en la novela sofía sigue recibiendo clases de filosofía y aluden al periodo de la ilustración también a kant y su deseo de unificar el pensamiento empírico y racional',\n",
              " 'en la noche del cumpleaños de sofía organizan una fiesta en el jardín de su casa',\n",
              " 'allí ocurren una serie de acontecimientos absurdos',\n",
              " 'alberto y sofía tratan de escapar del control de albert knag y desaparecen',\n",
              " 'cuando albert regresa de viaje le cuenta a su hija la última lección de filosofía mientras están sentados en un muelle',\n",
              " 'allí le habla sobre el big bang y la creación del universo',\n",
              " 'mientras sofía intenta intervenir en el mundo de hilde',\n",
              " 'la novela supone un recorrido a través de la historia de la filosofía occidental de manera cronológica',\n",
              " 'así este libro se convierte en una nueva oportunidad para aquellos que dieron la filosofía por imposible',\n",
              " 'mientras la protagonista evoluciona y se vuelve más segura los lectores aprenden sobre la filosofía y su importancia en el mundo',\n",
              " 'a continuación se puede observar un esquema de los filósofos que aparecen enel mundo de sofía',\n",
              " 'una de las cuestiones que plantea al lector es precisamente la delgada línea que existe entre ficción y realidad',\n",
              " '¿de qué forma el autor nos hace plantearnos incluso nuestra propia existencia',\n",
              " 'gracias a la metaficción a la creación de una novela dentro de una novela',\n",
              " 'sofía se plantea su existencia en el momento en el que descubre que puede ser solo parte de la imaginación de otra persona albert knag',\n",
              " 'esta es sin duda una de las preguntas más importantes que nos deja este libro',\n",
              " 'jostein gaarder propone a la filosofía como el camino fundamental para comprender la vida el que nos ayuda a comprender el origen de todas las cosas el mundo que nos rodea el lugar que ocupamos en él',\n",
              " 'como sofía a veces hemos tenido la sensación de que en el colegio no se enseña bien la filosofía y que incluso se le da más importancia a otras materias',\n",
              " 'este libro esconde una profunda crítica hacia los sistemas educativos que no dan la importancia que deberían a cuestiones más complejas cuyo estudio podría ser igual o más interesante que otras asignaturas corrientes',\n",
              " 'lo único que necesitamos para convertirnos en buenos filósofos es la capacidad de asombro',\n",
              " 'a veces los sistemas educativos se centran en desarrollar actividades rutinarias que merman la creatividad y la imaginación',\n",
              " 'el objetivo principal es superar la evaluación a través de pruebas escritas',\n",
              " '¿dónde queda entonces la capacidad de asombro del alumno',\n",
              " 'la primera pregunta que recibe sofía en una de las notas es “ ¿quién eres ”',\n",
              " 'inmediatamente el lector queda también sometido a esta cuestión',\n",
              " 'y es que al igual que la protagonista creemos saber quién somos',\n",
              " 'sin embargo lo que sofía desconoce es que es un personaje creado por el padre de hilde personaje a su vez de gaarder para que esta última aprenda filosofía',\n",
              " 'poco a poco a medida que sofía investiga sobre quién es hilde se descubre a sí misma como un personaje',\n",
              " 'es solo en este momento cuando conoce su identidad que deduce que el autoconocimiento es el único camino hacia la libertad y la búsqueda de la verdad',\n",
              " 'uno de los temas sometidos a estudio para algunos filósofos fue el destino',\n",
              " 'también es un tema primordial en este libro',\n",
              " '¿estamos condicionados por el destino o controlados por un ser superior',\n",
              " '¿actuamos libremente',\n",
              " 'podemos reflexionar sobre esta cuestión y objetar las diferentes posturas que tomaron los filósofos a cerca de este tema',\n",
              " 'pero hay una consideración de fondo que gaarder nos ofrece ¿y si al igual que sofía somos un personaje cuyas acciones están previamente establecidas o somos como ordenadores previamente programados',\n",
              " '¿por qué las mujeres que han contribuido en la historia del pensamiento no han tenido la suficiente repercusión o no son mencionadas en los manuales de filosofía',\n",
              " 'un tema interesante en el libro es el papel que la mujer ha tenido en la historia de la filosofía',\n",
              " 'en diferentes capítulos alberto knox hace referencia a que en la historia la mujer ha estado “ reprimida como ser pensante debido a su sexo ”',\n",
              " 'critica que las mujeres fueron tratadas como seres que cuya función principal era la de concebir',\n",
              " 'la elección del nombre de sofía para la protagonista de este libro no es algo casual',\n",
              " 'si atendemos a la etimología griega de la palabra sofía σoφíα significa sabiduría',\n",
              " 'sofía también era la diosa griega de la sabiduría',\n",
              " 'el propio término filosofía deriva de la raíz griega φιλος philos amor y σοφός sophos sabiduría',\n",
              " 'lo cual significa “ amor a la sabiduría ”',\n",
              " 'así lo que sofía simboliza y lo que se despierta en ella a lo largo de este libro es el “ amor ” precisamente al conocimiento',\n",
              " 'la primavera es la estación que está entre el invierno y el verano a menudo se suele asociar con el cambio la renovación',\n",
              " 'para los renacentistas la primavera era una metáfora del origen de la vida',\n",
              " 'en esta novela el contexto está enmarcado en esta estación que asociado al periodo de pubertad que experimenta la protagonista cobra un carácter simbólico',\n",
              " 'un periodo de transformación y cambio personal de crecimiento',\n",
              " 'en definitiva una renovación que va de la ingenuidad de un niño a la sabiduría de un adulto',\n",
              " 'en este proceso de madurez sofía sigue acudiendo a un sitio donde puede conservar su inocencia',\n",
              " 'como un niño que tiene un escondite secreto la joven accede a ese lugar a través de un seto al final del jardín de su casa',\n",
              " 'para sofía ese “ callejón ” es lo más parecido al paraíso tal y como ella imaginaba que sería el jardín del edén descrito en el génesis',\n",
              " 'durante la infancia es muy común que los niños vivan en un mundo de fantasía que luego pierden en la etapa adulta',\n",
              " 'así podemos entender el jardín como un lugar de protección de la niñez en el que sofía puede liberarse presión que tiene para crecer y perder su ingenuidad',\n",
              " 'en el libro existe una metáfora relacionada con el viejo truco del conejo blanco que sale de forma repentina del sombrero negro del mago',\n",
              " 'para gaarder el mundo es el conejo blanco',\n",
              " 'generalmente los adultos se esconden en el pelo del conejo y se mantienen de alguna forma ajenos al mundo',\n",
              " 'sin embargo establece una comparación entre los filósofos y los niños que se mantienen superiores al pelo del conejo',\n",
              " 'con esta explicación se mantiene la idea de que los filósofos y los niños están preparados a conocer el mundo que les rodea porque son capaces de mirar a los ojos del mago',\n",
              " 'es decir mantienen la curiosidad por responder a las preguntas sobre la vida',\n",
              " 'el espejo es una alusión directa a la novelaalicia a través del espejode lewis carroll',\n",
              " 'en el caso del libroel mundo de sofía el espejo sirve para unir a las dos protagonistas es como un “ portal ” que fusiona dos mundos paralelos el de hilde y el de sofía a través de su reflejo',\n",
              " 'es un escritor noruego autor del libroel mundo de sofía una de sus novelas más exitosas a nivel mundial desde su publicación en 1991',\n",
              " 'estudió lenguas escandinavas y teología también fue profesor de filosofíacomenzó su carrera literaria como escritor de cuentos infantiles',\n",
              " 'a principios de la década de los 90 obtuvo el premio nacional de crítica literaria de noruega más tarde también el premio europeo de literatura juvenilel mundo de sofíasupuso su reconocimiento a nivel mundial ya que la obra se tradujo en más de 40 idiomas y se convirtió en un auténticobest seller',\n",
              " 'en 1999 el director noruego erik gustavson llevó al cine la adaptación del libroel mundo de sofíade jostein gaarder',\n",
              " 'la película es un drama de aventuras protagonizado por la actriz silje storstein con una duración de 114 minutos pretende plasmar los acontecimientos más importantes de la novela',\n",
              " 'si ya disfrutaste de la novela aquí puedes ver eltráilerdel filmeel mundo de sofía',\n",
              " 'si te gustó este artículo también puedes leer']"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_token = []\n",
        "for a in range(0 , can):\n",
        "  n_token.append(preprocess_text(tokens_ora[a]))\n",
        "n_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otn_0j8gNMMt"
      },
      "outputs": [],
      "source": [
        "def get_max_length(array):\n",
        "  max= 0\n",
        "  for i in range(1,len(array)):\n",
        "    temp = len(array[i])\n",
        "    max =  temp if temp >= max else max\n",
        "  return max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcSzPEE_Ovml",
        "outputId": "b80ca7dc-256c-43ab-a8cb-77a5720b80ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "297"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_tam = get_max_length(n_token)\n",
        "max_tam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfe-_nbsS_IP"
      },
      "outputs": [],
      "source": [
        "# Importar las funciones tokenizer y pad_sequences de TensorFlow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOvZxbodSiYa"
      },
      "outputs": [],
      "source": [
        "# definimos la instancia para implementar la función tokenizer de TF\n",
        "# instancia = función tokenizer (máximo de palabras en el diccionario, token para palabras que no estén en el índice)\n",
        "#tokenizador = Tokenizer(num_words = 100, oov_token=\"<OOV>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RUtyoKIQUmL"
      },
      "outputs": [],
      "source": [
        "tokenizador = Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGW__U0FQIdi",
        "outputId": "f92baca4-1ee3-4462-c154-24681fe1d104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'de': 1, 'la': 2, 'que': 3, 'el': 4, 'a': 5, 'en': 6, 'y': 7, 'sofía': 8, 'un': 9, 'es': 10, 'su': 11, 'una': 12, 'filosofía': 13, 'del': 14, 'los': 15, 'se': 16, 'las': 17, 'mundo': 18, 'como': 19, 'al': 20, 'para': 21, 'con': 22, 'por': 23, 'sobre': 24, 'también': 25, 'hilde': 26, 'más': 27, 'este': 28, 'novela': 29, 'esta': 30, 'le': 31, 'no': 32, 'libro': 33, 'profesor': 34, 'lo': 35, 'través': 36, 'joven': 37, 'alberto': 38, 'filósofos': 39, 'gaarder': 40, 'puede': 41, 'recibe': 42, 'albert': 43, 'historia': 44, 'o': 45, 'solo': 46, 'protagonista': 47, 'knag': 48, 'descubre': 49, 'allí': 50, 'si': 51, 'nos': 52, 'somos': 53, 'son': 54, 'está': 55, 'encuentra': 56, 'así': 57, 'después': 58, 'están': 59, 'da': 60, 'casa': 61, 'espejo': 62, 'lugar': 63, 'jardín': 64, 'ser': 65, 'personaje': 66, 'sabiduría': 67, 'conejo': 68, 'jostein': 69, 'preguntas': 70, 'qué': 71, 'destino': 72, 'cuestiones': 73, 'autor': 74, 'lector': 75, 'curso': 76, 'primera': 77, 'comienza': 78, 'knox': 79, 'cuyo': 80, 'padre': 81, 'identidad': 82, 'lección': 83, 'antigua': 84, 'habla': 85, 'última': 86, 'durante': 87, 'cuando': 88, 'tiene': 89, 'fue': 90, 'ha': 91, 'hija': 92, 'momento': 93, 'hace': 94, 'existe': 95, 'periodo': 96, 'mientras': 97, 'importancia': 98, 'entre': 99, 'forma': 100, 'sin': 101, 'vida': 102, 'tenido': 103, 'igual': 104, 'tema': 105, 'era': 106, 'griega': 107, 'niños': 108, 'mantienen': 109, 'libroel': 110, '1991': 111, 'obra': 112, 'responder': 113, '¿por': 114, 'estamos': 115, 'aquí': 116, 'reflexionar': 117, 'tiempo': 118, 'ojos': 119, 'noruega': 120, 'punto': 121, 'día': 122, 'colegio': 123, 'leer': 124, '“¿quién': 125, 'eres': 126, '”': 127, 'otro': 128, 'postales': 129, 'desconoce': 130, 'humanos': 131, 'explicación': 132, 'cual': 133, 'conocer': 134, 'famosa': 135, 'frase': 136, 'sé': 137, 'teoría': 138, 'bosque': 139, 'reflejo': 140, 'aprende': 141, 'religión': 142, 'luego': 143, 'tarde': 144, 'creado': 145, 'entender': 146, 'viaje': 147, 'cumpleaños': 148, 'tal': 149, 'crucifijo': 150, 'cuenta': 151, 'verdad': 152, 'sigue': 153, 'pensamiento': 154, 'acontecimientos': 155, 'creación': 156, 'plantea': 157, 'precisamente': 158, 'incluso': 159, 'existencia': 160, 'imaginación': 161, 'importantes': 162, 'camino': 163, 'comprender': 164, 'origen': 165, 'rodea': 166, 'veces': 167, 'otras': 168, 'crítica': 169, 'hacia': 170, 'sistemas': 171, 'educativos': 172, 'estudio': 173, 'interesante': 174, 'único': 175, 'capacidad': 176, 'asombro': 177, 'principal': 178, 'queda': 179, 'cuestión': 180, 'quién': 181, 'embargo': 182, 'poco': 183, 'podemos': 184, 'diferentes': 185, 'previamente': 186, 'mujeres': 187, 'han': 188, 'mujer': 189, 'significa': 190, 'ella': 191, 'primavera': 192, 'estación': 193, 'cambio': 194, 'renovación': 195, 'metáfora': 196, 'ingenuidad': 197, 'niño': 198, 'ese': 199, 'blanco': 200, 'mago': 201, 'pelo': 202, 'dos': 203, 'escritor': 204, 'noruego': 205, 'nivel': 206, 'mundial': 207, 'literaria': 208, 'premio': 209, 'ya': 210, 'puedes': 211, 'escrita': 212, 'trata': 213, 'trascendentales': 214, '¿quiénes': 215, '¿existe': 216, 'libre': 217, 'albedrío': 218, 'determinados': 219, 'estas': 220, 'algunas': 221, 'invita': 222, 'mismo': 223, 'asiste': 224, 'inmejorable': 225, 'cumplir': 226, '15': 227, 'años': 228, 'regresar': 229, 'buzón': 230, 'nota': 231, 'carta': 232, 'muchas': 233, 'distancia': 234, 'impartido': 235, 'misterioso': 236, 'lado': 237, 'destinadas': 238, 'misteriosa': 239, 'muchacha': 240, 'llamada': 241, 'remite': 242, 'aunque': 243, 'principio': 244, 'cómo': 245, 'surgieron': 246, 'mitos': 247, 'primeros': 248, 'aludían': 249, 'ellos': 250, 'explicar': 251, 'fenómenos': 252, 'naturales': 253, 'aparecieron': 254, 'denominados': 255, '“filósofos': 256, 'naturaleza”': 257, 'trataron': 258, 'dar': 259, 'poner': 260, 'evidencia': 261, 'creencias': 262, 'mitológicas': 263, 'imperantes': 264, 'siguientes': 265, 'lecciones': 266, 'aborda': 267, 'relacionadas': 268, 'entretanto': 269, 'verdadera': 270, 'bufanda': 271, 'roja': 272, 'antigüedad': 273, 'sócrates': 274, '“solo': 275, 'nada”': 276, 'platón': 277, 'ideas': 278, 'mito': 279, 'caverna': 280, 'cinta': 281, 'vídeo': 282, 'recorre': 283, 'calles': 284, 'atenas': 285, 'paseo': 286, '“cabaña': 287, 'mayor”': 288, 'esa': 289, 've': 290, 'devuelve': 291, 'guiño': 292, 'finalmente': 293, 'toma': 294, 'clase': 295, 'aristóteles': 296, 'unos': 297, 'días': 298, 'cartas': 299, 'decide': 300, 'ir': 301, 'amiga': 302, 'cabaña': 303, 'encuentran': 304, 'marchan': 305, 'lleva': 306, 'siguiente': 307, 'cita': 308, 'iglesia': 309, 'edad': 310, 'media': 311, 'notoria': 312, 'influencia': 313, 'san': 314, 'agustín': 315, 'santo': 316, 'tomás': 317, 'aquino': 318, 'quienes': 319, 'terminaron': 320, '“hermanar”': 321, 'pasa': 322, 'explicarle': 323, 'figuras': 324, 'clave': 325, 'renacimiento': 326, 'barroco': 327, 'dualista': 328, 'descartes': 329, '“pienso': 330, 'existo”': 331, 'continúa': 332, 'spinoza': 333, 'creyó': 334, 'dios': 335, 'había': 336, 'leyes': 337, 'naturaleza': 338, 'rigen': 339, 'hombre': 340, 'hacer': 341, 'locke': 342, '“tabula': 343, 'rasa”': 344, 'david': 345, 'hume': 346, 'george': 347, 'berkerley': 348, 'pensó': 349, 'existen': 350, 'mente': 351, '“dios': 352, 'creador”': 353, 'personajes': 354, 'ficticios': 355, 'escrito': 356, 'trabajo': 357, 'envía': 358, 'llamadoel': 359, 'sofíacomo': 360, 'regalo': 361, 'leerlo': 362, 'obsesiona': 363, 'hasta': 364, 'pierde': 365, 'noción': 366, 'sueña': 367, 'busca': 368, 'hecho': 369, 'plantearse': 370, 'recibiendo': 371, 'clases': 372, 'aluden': 373, 'ilustración': 374, 'kant': 375, 'deseo': 376, 'unificar': 377, 'empírico': 378, 'racional': 379, 'noche': 380, 'organizan': 381, 'fiesta': 382, 'ocurren': 383, 'serie': 384, 'absurdos': 385, 'tratan': 386, 'escapar': 387, 'control': 388, 'desaparecen': 389, 'regresa': 390, 'sentados': 391, 'muelle': 392, 'big': 393, 'bang': 394, 'universo': 395, 'intenta': 396, 'intervenir': 397, 'supone': 398, 'recorrido': 399, 'occidental': 400, 'manera': 401, 'cronológica': 402, 'convierte': 403, 'nueva': 404, 'oportunidad': 405, 'aquellos': 406, 'dieron': 407, 'imposible': 408, 'evoluciona': 409, 'vuelve': 410, 'segura': 411, 'lectores': 412, 'aprenden': 413, 'continuación': 414, 'observar': 415, 'esquema': 416, 'aparecen': 417, 'enel': 418, 'delgada': 419, 'línea': 420, 'ficción': 421, 'realidad': 422, '¿de': 423, 'plantearnos': 424, 'nuestra': 425, 'propia': 426, 'gracias': 427, 'metaficción': 428, 'dentro': 429, 'parte': 430, 'otra': 431, 'persona': 432, 'duda': 433, 'deja': 434, 'propone': 435, 'fundamental': 436, 'ayuda': 437, 'todas': 438, 'cosas': 439, 'ocupamos': 440, 'él': 441, 'hemos': 442, 'sensación': 443, 'enseña': 444, 'bien': 445, 'materias': 446, 'esconde': 447, 'profunda': 448, 'dan': 449, 'deberían': 450, 'complejas': 451, 'podría': 452, 'asignaturas': 453, 'corrientes': 454, 'necesitamos': 455, 'convertirnos': 456, 'buenos': 457, 'centran': 458, 'desarrollar': 459, 'actividades': 460, 'rutinarias': 461, 'merman': 462, 'creatividad': 463, 'objetivo': 464, 'superar': 465, 'evaluación': 466, 'pruebas': 467, 'escritas': 468, '¿dónde': 469, 'entonces': 470, 'alumno': 471, 'pregunta': 472, 'notas': 473, 'inmediatamente': 474, 'sometido': 475, 'creemos': 476, 'saber': 477, 'vez': 478, 'aprenda': 479, 'medida': 480, 'investiga': 481, 'sí': 482, 'misma': 483, 'conoce': 484, 'deduce': 485, 'autoconocimiento': 486, 'libertad': 487, 'búsqueda': 488, 'uno': 489, 'temas': 490, 'sometidos': 491, 'algunos': 492, 'primordial': 493, '¿estamos': 494, 'condicionados': 495, 'controlados': 496, 'superior': 497, '¿actuamos': 498, 'libremente': 499, 'objetar': 500, 'posturas': 501, 'tomaron': 502, 'cerca': 503, 'pero': 504, 'hay': 505, 'consideración': 506, 'fondo': 507, 'ofrece': 508, '¿y': 509, 'cuyas': 510, 'acciones': 511, 'establecidas': 512, 'ordenadores': 513, 'programados': 514, 'contribuido': 515, 'suficiente': 516, 'repercusión': 517, 'mencionadas': 518, 'manuales': 519, 'papel': 520, 'capítulos': 521, 'referencia': 522, 'estado': 523, '“reprimida': 524, 'pensante': 525, 'debido': 526, 'sexo”': 527, 'critica': 528, 'fueron': 529, 'tratadas': 530, 'seres': 531, 'cuya': 532, 'función': 533, 'concebir': 534, 'elección': 535, 'nombre': 536, 'algo': 537, 'casual': 538, 'atendemos': 539, 'etimología': 540, 'palabra': 541, 'σoφíα': 542, 'diosa': 543, 'propio': 544, 'término': 545, 'deriva': 546, 'raíz': 547, 'φιλος': 548, 'philos': 549, 'amor': 550, 'σοφός': 551, 'sophos': 552, '“amor': 553, 'sabiduría”': 554, 'simboliza': 555, 'despierta': 556, 'largo': 557, '“amor”': 558, 'conocimiento': 559, 'invierno': 560, 'verano': 561, 'menudo': 562, 'suele': 563, 'asociar': 564, 'renacentistas': 565, 'contexto': 566, 'enmarcado': 567, 'asociado': 568, 'pubertad': 569, 'experimenta': 570, 'cobra': 571, 'carácter': 572, 'simbólico': 573, 'transformación': 574, 'personal': 575, 'crecimiento': 576, 'definitiva': 577, 'va': 578, 'adulto': 579, 'proceso': 580, 'madurez': 581, 'acudiendo': 582, 'sitio': 583, 'donde': 584, 'conservar': 585, 'inocencia': 586, 'escondite': 587, 'secreto': 588, 'accede': 589, 'seto': 590, 'final': 591, '“callejón”': 592, 'parecido': 593, 'paraíso': 594, 'imaginaba': 595, 'sería': 596, 'edén': 597, 'descrito': 598, 'génesis': 599, 'infancia': 600, 'muy': 601, 'común': 602, 'vivan': 603, 'fantasía': 604, 'pierden': 605, 'etapa': 606, 'adulta': 607, 'protección': 608, 'niñez': 609, 'liberarse': 610, 'presión': 611, 'crecer': 612, 'perder': 613, 'relacionada': 614, 'viejo': 615, 'truco': 616, 'sale': 617, 'repentina': 618, 'sombrero': 619, 'negro': 620, 'generalmente': 621, 'adultos': 622, 'esconden': 623, 'alguna': 624, 'ajenos': 625, 'establece': 626, 'comparación': 627, 'superiores': 628, 'mantiene': 629, 'idea': 630, 'preparados': 631, 'les': 632, 'porque': 633, 'capaces': 634, 'mirar': 635, 'decir': 636, 'curiosidad': 637, 'alusión': 638, 'directa': 639, 'novelaalicia': 640, 'espejode': 641, 'lewis': 642, 'carroll': 643, 'caso': 644, 'sirve': 645, 'unir': 646, 'protagonistas': 647, '“portal”': 648, 'fusiona': 649, 'mundos': 650, 'paralelos': 651, 'sus': 652, 'novelas': 653, 'exitosas': 654, 'desde': 655, 'publicación': 656, 'estudió': 657, 'lenguas': 658, 'escandinavas': 659, 'teología': 660, 'comenzó': 661, 'carrera': 662, 'cuentos': 663, 'infantiles': 664, 'principios': 665, 'década': 666, '90': 667, 'obtuvo': 668, 'nacional': 669, 'europeo': 670, 'literatura': 671, 'juvenil': 672, 'sofíasupuso': 673, 'reconocimiento': 674, 'tradujo': 675, '40': 676, 'idiomas': 677, 'convirtió': 678, 'auténticobest': 679, 'seller': 680, '1999': 681, 'director': 682, 'erik': 683, 'gustavson': 684, 'llevó': 685, 'cine': 686, 'adaptación': 687, 'sofíade': 688, 'película': 689, 'drama': 690, 'aventuras': 691, 'protagonizado': 692, 'actriz': 693, 'silje': 694, 'storstein': 695, 'duración': 696, '114': 697, 'minutos': 698, 'pretende': 699, 'plasmar': 700, 'disfrutaste': 701, 'ver': 702, 'eltráilerdel': 703, 'filmeel': 704, 'te': 705, 'gustó': 706, 'artículo': 707}\n"
          ]
        }
      ],
      "source": [
        "# aplicar a la instancia \"tokenizador\" creada previamente la función .fit_on_text en el corpus\n",
        "tokenizador.fit_on_texts(tokens_ora)\n",
        "\n",
        "# crear objeto para almacenar los índices de los tokens\n",
        "word_index = tokenizador.word_index\n",
        "\n",
        "# mostrar objeto creado\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrZvaZgaTJrv",
        "outputId": "eecf739b-a2fa-43b8-cd04-f19e79830d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[4, 18, 1, 8, 111, 10, 12, 29, 212, 23, 69, 40], [30, 112, 213, 1, 113, 5, 70, 214, 19, 215, 53], [114, 71, 115, 116], [216, 4, 217, 218, 45, 115, 219, 23, 4, 72], [220, 54, 46, 221, 1, 17, 73, 22, 17, 3, 4, 74, 222, 20, 75, 5, 117, 20, 223, 118, 3, 224, 5, 9, 76, 1, 13, 225, 5, 36, 1, 15, 119, 1, 8, 11, 47], [8, 10, 12, 37, 120, 3, 55, 5, 121, 1, 226, 227, 228], [9, 122, 20, 229, 14, 123, 56, 6, 4, 230, 12, 231, 6, 2, 3, 41, 124, 125, 126, 127], [30, 10, 2, 77, 232, 1, 17, 233, 3, 42], [57, 10, 19, 2, 37, 78, 9, 76, 1, 13, 5, 234, 235, 23, 9, 236, 34, 38, 79], [23, 128, 237, 25, 42, 129, 238, 5, 12, 239, 240, 241, 26, 80, 242, 10, 43, 48, 11, 81], [8, 78, 4, 76, 1, 13, 243, 20, 244, 130, 2, 82, 1, 11, 34], [2, 77, 83, 3, 42, 10, 24, 245, 246, 15, 247], [15, 248, 131, 249, 5, 250, 21, 251, 15, 252, 253], [58, 254, 15, 255, 19, 256, 1, 257, 3, 258, 1, 259, 12, 132, 24, 4, 18, 7, 260, 6, 261, 17, 262, 263, 264], [17, 265, 266, 3, 267, 4, 34, 59, 268, 22, 2, 13, 84], [269, 2, 37, 8, 49, 133, 10, 2, 270, 82, 1, 38, 79, 7, 25, 16, 56, 22, 12, 271, 272, 1, 26], [21, 134, 2, 13, 1, 2, 273, 38, 31, 85, 1, 274, 7, 11, 135, 136, 275, 137, 3, 32, 137, 276], [128, 122, 8, 42, 12, 83, 24, 277, 11, 138, 1, 17, 278, 7, 4, 279, 1, 2, 280, 5, 36, 1, 12, 281, 1, 282, 6, 2, 3, 4, 34, 283, 17, 284, 1, 2, 84, 285], [58, 8, 60, 9, 286, 23, 4, 139, 7, 16, 56, 22, 2, 287, 14, 288, 7, 49, 3, 289, 10, 2, 61, 1, 11, 34, 1, 13], [50, 290, 9, 62, 80, 140, 31, 291, 9, 292], [293, 2, 37, 294, 2, 86, 295, 1, 13, 84, 7, 141, 24, 296], [87, 297, 298, 8, 32, 42, 299, 1, 38, 7, 300, 301, 22, 11, 302, 5, 2, 303, 14, 139], [50, 304, 129, 21, 26], [88, 16, 305, 2, 37, 16, 306, 4, 62, 5, 11, 61], [2, 307, 308, 22, 4, 34, 1, 13, 89, 63, 6, 12, 309], [50, 141, 3, 87, 2, 310, 311, 90, 312, 2, 313, 1, 2, 142, 6, 2, 13], [2, 37, 49, 5, 39, 19, 314, 315, 7, 5, 316, 317, 1, 318, 319, 320, 23, 321, 5, 2, 13, 7, 2, 142], [4, 34, 322, 5, 323, 5, 8, 17, 324, 325, 14, 326, 7, 4, 327], [25, 2, 13, 328, 1, 329, 7, 11, 135, 136, 330, 143, 331], [27, 144, 332, 22, 2, 13, 1, 333, 3, 334, 3, 335, 336, 145, 17, 337, 1, 2, 338, 3, 339, 35, 3, 4, 340, 41, 341], [58, 31, 85, 24, 342, 7, 11, 138, 24, 2, 343, 344, 25, 24, 345, 346, 7, 347, 348, 3, 349, 3, 15, 131, 46, 350, 6, 2, 351, 1, 9, 352, 353], [38, 31, 60, 5, 146, 5, 8, 3, 54, 354, 355, 1, 12, 29, 3, 43, 48, 91, 356, 21, 11, 92, 26], [43, 48, 3, 16, 56, 1, 147, 1, 357, 31, 358, 5, 11, 92, 9, 33, 359, 18, 1, 360, 361, 1, 148], [26, 78, 5, 362, 7, 16, 363, 22, 2, 44, 364, 149, 121, 3, 365, 2, 366, 14, 118], [6, 9, 93, 1, 2, 29, 8, 367, 22, 26, 7, 11, 150], [58, 26, 368, 4, 150, 7, 16, 60, 151, 3, 32, 55], [28, 369, 31, 94, 370, 51, 8, 95, 1, 152, 7, 32, 46, 6, 2, 44, 1, 11, 81], [6, 2, 29, 8, 153, 371, 372, 1, 13, 7, 373, 20, 96, 1, 2, 374, 25, 5, 375, 7, 11, 376, 1, 377, 4, 154, 378, 7, 379], [6, 2, 380, 14, 148, 1, 8, 381, 12, 382, 6, 4, 64, 1, 11, 61], [50, 383, 12, 384, 1, 155, 385], [38, 7, 8, 386, 1, 387, 14, 388, 1, 43, 48, 7, 389], [88, 43, 390, 1, 147, 31, 151, 5, 11, 92, 2, 86, 83, 1, 13, 97, 59, 391, 6, 9, 392], [50, 31, 85, 24, 4, 393, 394, 7, 2, 156, 14, 395], [97, 8, 396, 397, 6, 4, 18, 1, 26], [2, 29, 398, 9, 399, 5, 36, 1, 2, 44, 1, 2, 13, 400, 1, 401, 402], [57, 28, 33, 16, 403, 6, 12, 404, 405, 21, 406, 3, 407, 2, 13, 23, 408], [97, 2, 47, 409, 7, 16, 410, 27, 411, 15, 412, 413, 24, 2, 13, 7, 11, 98, 6, 4, 18], [5, 414, 16, 41, 415, 9, 416, 1, 15, 39, 3, 417, 418, 18, 1, 8], [12, 1, 17, 73, 3, 157, 20, 75, 10, 158, 2, 419, 420, 3, 95, 99, 421, 7, 422], [423, 71, 100, 4, 74, 52, 94, 424, 159, 425, 426, 160], [427, 5, 2, 428, 5, 2, 156, 1, 12, 29, 429, 1, 12, 29], [8, 16, 157, 11, 160, 6, 4, 93, 6, 4, 3, 49, 3, 41, 65, 46, 430, 1, 2, 161, 1, 431, 432, 43, 48], [30, 10, 101, 433, 12, 1, 17, 70, 27, 162, 3, 52, 434, 28, 33], [69, 40, 435, 5, 2, 13, 19, 4, 163, 436, 21, 164, 2, 102, 4, 3, 52, 437, 5, 164, 4, 165, 1, 438, 17, 439, 4, 18, 3, 52, 166, 4, 63, 3, 440, 6, 441], [19, 8, 5, 167, 442, 103, 2, 443, 1, 3, 6, 4, 123, 32, 16, 444, 445, 2, 13, 7, 3, 159, 16, 31, 60, 27, 98, 5, 168, 446], [28, 33, 447, 12, 448, 169, 170, 15, 171, 172, 3, 32, 449, 2, 98, 3, 450, 5, 73, 27, 451, 80, 173, 452, 65, 104, 45, 27, 174, 3, 168, 453, 454], [35, 175, 3, 455, 21, 456, 6, 457, 39, 10, 2, 176, 1, 177], [5, 167, 15, 171, 172, 16, 458, 6, 459, 460, 461, 3, 462, 2, 463, 7, 2, 161], [4, 464, 178, 10, 465, 2, 466, 5, 36, 1, 467, 468], [469, 179, 470, 2, 176, 1, 177, 14, 471], [2, 77, 472, 3, 42, 8, 6, 12, 1, 17, 473, 10, 125, 126, 127], [474, 4, 75, 179, 25, 475, 5, 30, 180], [7, 10, 3, 20, 104, 3, 2, 47, 476, 477, 181, 53], [101, 182, 35, 3, 8, 130, 10, 3, 10, 9, 66, 145, 23, 4, 81, 1, 26, 66, 5, 11, 478, 1, 40, 21, 3, 30, 86, 479, 13], [183, 5, 183, 5, 480, 3, 8, 481, 24, 181, 10, 26, 16, 49, 5, 482, 483, 19, 9, 66], [10, 46, 6, 28, 93, 88, 484, 11, 82, 3, 485, 3, 4, 486, 10, 4, 175, 163, 170, 2, 487, 7, 2, 488, 1, 2, 152], [489, 1, 15, 490, 491, 5, 173, 21, 492, 39, 90, 4, 72], [25, 10, 9, 105, 493, 6, 28, 33], [494, 495, 23, 4, 72, 45, 496, 23, 9, 65, 497], [498, 499], [184, 117, 24, 30, 180, 7, 500, 17, 185, 501, 3, 502, 15, 39, 5, 503, 1, 28, 105], [504, 505, 12, 506, 1, 507, 3, 40, 52, 508, 509, 51, 20, 104, 3, 8, 53, 9, 66, 510, 511, 59, 186, 512, 45, 53, 19, 513, 186, 514], [114, 71, 17, 187, 3, 188, 515, 6, 2, 44, 14, 154, 32, 188, 103, 2, 516, 517, 45, 32, 54, 518, 6, 15, 519, 1, 13], [9, 105, 174, 6, 4, 33, 10, 4, 520, 3, 2, 189, 91, 103, 6, 2, 44, 1, 2, 13], [6, 185, 521, 38, 79, 94, 522, 5, 3, 6, 2, 44, 2, 189, 91, 523, 524, 19, 65, 525, 526, 5, 11, 527], [528, 3, 17, 187, 529, 530, 19, 531, 3, 532, 533, 178, 106, 2, 1, 534], [2, 535, 14, 536, 1, 8, 21, 2, 47, 1, 28, 33, 32, 10, 537, 538], [51, 539, 5, 2, 540, 107, 1, 2, 541, 8, 542, 190, 67], [8, 25, 106, 2, 543, 107, 1, 2, 67], [4, 544, 545, 13, 546, 1, 2, 547, 107, 548, 549, 550, 7, 551, 552, 67], [35, 133, 190, 553, 5, 2, 554], [57, 35, 3, 8, 555, 7, 35, 3, 16, 556, 6, 191, 5, 35, 557, 1, 28, 33, 10, 4, 558, 158, 20, 559], [2, 192, 10, 2, 193, 3, 55, 99, 4, 560, 7, 4, 561, 5, 562, 16, 563, 564, 22, 4, 194, 2, 195], [21, 15, 565, 2, 192, 106, 12, 196, 14, 165, 1, 2, 102], [6, 30, 29, 4, 566, 55, 567, 6, 30, 193, 3, 568, 20, 96, 1, 569, 3, 570, 2, 47, 571, 9, 572, 573], [9, 96, 1, 574, 7, 194, 575, 1, 576], [6, 577, 12, 195, 3, 578, 1, 2, 197, 1, 9, 198, 5, 2, 67, 1, 9, 579], [6, 28, 580, 1, 581, 8, 153, 582, 5, 9, 583, 584, 41, 585, 11, 586], [19, 9, 198, 3, 89, 9, 587, 588, 2, 37, 589, 5, 199, 63, 5, 36, 1, 9, 590, 20, 591, 14, 64, 1, 11, 61], [21, 8, 199, 592, 10, 35, 27, 593, 20, 594, 149, 7, 19, 191, 595, 3, 596, 4, 64, 14, 597, 598, 6, 4, 599], [87, 2, 600, 10, 601, 602, 3, 15, 108, 603, 6, 9, 18, 1, 604, 3, 143, 605, 6, 2, 606, 607], [57, 184, 146, 4, 64, 19, 9, 63, 1, 608, 1, 2, 609, 6, 4, 3, 8, 41, 610, 611, 3, 89, 21, 612, 7, 613, 11, 197], [6, 4, 33, 95, 12, 196, 614, 22, 4, 615, 616, 14, 68, 200, 3, 617, 1, 100, 618, 14, 619, 620, 14, 201], [21, 40, 4, 18, 10, 4, 68, 200], [621, 15, 622, 16, 623, 6, 4, 202, 14, 68, 7, 16, 109, 1, 624, 100, 625, 20, 18], [101, 182, 626, 12, 627, 99, 15, 39, 7, 15, 108, 3, 16, 109, 628, 20, 202, 14, 68], [22, 30, 132, 16, 629, 2, 630, 1, 3, 15, 39, 7, 15, 108, 59, 631, 5, 134, 4, 18, 3, 632, 166, 633, 54, 634, 1, 635, 5, 15, 119, 14, 201], [10, 636, 109, 2, 637, 23, 113, 5, 17, 70, 24, 2, 102], [4, 62, 10, 12, 638, 639, 5, 2, 640, 5, 36, 14, 641, 642, 643], [6, 4, 644, 14, 110, 18, 1, 8, 4, 62, 645, 21, 646, 5, 17, 203, 647, 10, 19, 9, 648, 3, 649, 203, 650, 651, 4, 1, 26, 7, 4, 1, 8, 5, 36, 1, 11, 140], [10, 9, 204, 205, 74, 14, 110, 18, 1, 8, 12, 1, 652, 653, 27, 654, 5, 206, 207, 655, 11, 656, 6, 111], [657, 658, 659, 7, 660, 25, 90, 34, 1, 13, 661, 11, 662, 208, 19, 204, 1, 663, 664], [5, 665, 1, 2, 666, 1, 15, 667, 668, 4, 209, 669, 1, 169, 208, 1, 120, 27, 144, 25, 4, 209, 670, 1, 671, 672, 4, 18, 1, 673, 11, 674, 5, 206, 207, 210, 3, 2, 112, 16, 675, 6, 27, 1, 676, 677, 7, 16, 678, 6, 9, 679, 680], [6, 681, 4, 682, 205, 683, 684, 685, 20, 686, 2, 687, 14, 110, 18, 1, 688, 69, 40], [2, 689, 10, 9, 690, 1, 691, 692, 23, 2, 693, 694, 695, 22, 12, 696, 1, 697, 698, 699, 700, 15, 155, 27, 162, 1, 2, 29], [51, 210, 701, 1, 2, 29, 116, 211, 702, 703, 704, 18, 1, 8], [51, 705, 706, 28, 707, 25, 211, 124]]\n"
          ]
        }
      ],
      "source": [
        "# Crearemos instancia para convertir las oraciones en sequencias\n",
        "sequencias = tokenizador.texts_to_sequences(tokens_ora)\n",
        "print (sequencias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5RjLUjgvhst"
      },
      "outputs": [],
      "source": [
        "# max_tam = get_max_length(sequencias)\n",
        "# max_tam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO0jnQkvsMrP"
      },
      "outputs": [],
      "source": [
        "def secuence_token(tokens, sequencia, tokenizador):\n",
        "  output_sequence = []\n",
        "  for l in tokens:\n",
        "    token_list = tokenizador.texts_to_sequences([l])[0]\n",
        "    for i in range(1 ,  len(token_list)):\n",
        "        # establecer token tipo Ngram\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        # Almacenar Ngramas creados\n",
        "        output_sequence.append(n_gram_sequence)\n",
        "  return output_sequence\n",
        "sequence_2 = secuence_token(tokens_ora, sequencias, tokenizador)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bvUMLhJ8p_p",
        "outputId": "cd4c6a99-be3f-4096-cb11-b3b33b4e29d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_tam = get_max_length(sequence_2)\n",
        "max_tam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsYLa4KETPsQ",
        "outputId": "1c1efd32-333d-45eb-cff8-7050af916dd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,   4,  18],\n",
              "       [  0,   0,   0, ...,   4,  18,   1],\n",
              "       [  0,   0,   0, ...,  18,   1,   8],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,  28, 707,  25],\n",
              "       [  0,   0,   0, ..., 707,  25, 211],\n",
              "       [  0,   0,   0, ...,  25, 211, 124]], dtype=int32)"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Crearemos instancia para aplicar la función \"pad_sequences\" de TF a las sequencias del texto\n",
        "padd = pad_sequences(sequence_2, maxlen=max_tam, padding='pre')\n",
        "padd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3kF32MyyeRK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow.keras.utils as ku\n",
        "input_sec = np.array(padd)\n",
        "\n",
        "predictors, label = input_sec[:,:-1],input_sec[:,-1]\n",
        "label = ku.to_categorical(label, num_classes= (len(word_index) + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jyd4raUssBYK",
        "outputId": "5e8ceeab-3d8b-413e-d163-93918e20edc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 52, 10)            7080      \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 128)               71168     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 708)               91332     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 169580 (662.42 KB)\n",
            "Trainable params: 169580 (662.42 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout\n",
        "# definimos función \"create_model\" max sequence y total words\n",
        "def create_model(max_sequence_len, total_words):\n",
        "    # Dimensión de la entrada\n",
        "    input_len = max_sequence_len - 1\n",
        "    # tipo del modelo\n",
        "    model = Sequential()\n",
        "\n",
        "    # adicionamos capa tipo Embedding)\n",
        "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
        "    #model.add(Embedding(total_words, 10, input_length=input_len))\n",
        "\n",
        "    # adicionamos capa tipo LSTM\n",
        "    model.add(LSTM(128))\n",
        "    # adicionamos capa de dropout\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    # adicionamos capa Densa de salida\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    # definimos parametros de .compile para el modelo\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = 'accuracy')\n",
        "    return model\n",
        "print(max_tam);\n",
        "model = create_model(max_tam, len(word_index) + 1)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jLaIyYZx-5z",
        "outputId": "88a85753-8fdc-4bd1-a105-4d7f53c39045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/800\n",
            "59/59 [==============================] - 14s 134ms/step - loss: 6.0660 - accuracy: 0.0567\n",
            "Epoch 2/800\n",
            "59/59 [==============================] - 9s 160ms/step - loss: 5.6040 - accuracy: 0.0589\n",
            "Epoch 3/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 5.5475 - accuracy: 0.0567\n",
            "Epoch 4/800\n",
            "59/59 [==============================] - 6s 101ms/step - loss: 5.5192 - accuracy: 0.0600\n",
            "Epoch 5/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 5.4726 - accuracy: 0.0658\n",
            "Epoch 6/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 5.4157 - accuracy: 0.0648\n",
            "Epoch 7/800\n",
            "59/59 [==============================] - 5s 92ms/step - loss: 5.3589 - accuracy: 0.0637\n",
            "Epoch 8/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 5.3063 - accuracy: 0.0707\n",
            "Epoch 9/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 5.2595 - accuracy: 0.0776\n",
            "Epoch 10/800\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 5.2116 - accuracy: 0.0846\n",
            "Epoch 11/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 5.1599 - accuracy: 0.0910\n",
            "Epoch 12/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 5.0932 - accuracy: 0.0996\n",
            "Epoch 13/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 5.0351 - accuracy: 0.1001\n",
            "Epoch 14/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 4.9647 - accuracy: 0.0964\n",
            "Epoch 15/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 4.9013 - accuracy: 0.0990\n",
            "Epoch 16/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 4.8304 - accuracy: 0.0964\n",
            "Epoch 17/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 4.7671 - accuracy: 0.1012\n",
            "Epoch 18/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 4.7121 - accuracy: 0.1001\n",
            "Epoch 19/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 4.6530 - accuracy: 0.1076\n",
            "Epoch 20/800\n",
            "59/59 [==============================] - 5s 89ms/step - loss: 4.5963 - accuracy: 0.1044\n",
            "Epoch 21/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 4.5375 - accuracy: 0.1071\n",
            "Epoch 22/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 4.4826 - accuracy: 0.1071\n",
            "Epoch 23/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 4.4308 - accuracy: 0.1092\n",
            "Epoch 24/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 4.3717 - accuracy: 0.1135\n",
            "Epoch 25/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 4.3244 - accuracy: 0.1188\n",
            "Epoch 26/800\n",
            "59/59 [==============================] - 4s 59ms/step - loss: 4.2657 - accuracy: 0.1221\n",
            "Epoch 27/800\n",
            "59/59 [==============================] - 6s 102ms/step - loss: 4.2125 - accuracy: 0.1215\n",
            "Epoch 28/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 4.1565 - accuracy: 0.1274\n",
            "Epoch 29/800\n",
            "59/59 [==============================] - 4s 59ms/step - loss: 4.1039 - accuracy: 0.1258\n",
            "Epoch 30/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 4.0466 - accuracy: 0.1370\n",
            "Epoch 31/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 3.9972 - accuracy: 0.1376\n",
            "Epoch 32/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 3.9317 - accuracy: 0.1397\n",
            "Epoch 33/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 3.8840 - accuracy: 0.1467\n",
            "Epoch 34/800\n",
            "59/59 [==============================] - 5s 91ms/step - loss: 3.8315 - accuracy: 0.1472\n",
            "Epoch 35/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 3.7740 - accuracy: 0.1552\n",
            "Epoch 36/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 3.7134 - accuracy: 0.1665\n",
            "Epoch 37/800\n",
            "59/59 [==============================] - 5s 94ms/step - loss: 3.6544 - accuracy: 0.1660\n",
            "Epoch 38/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 3.5982 - accuracy: 0.1681\n",
            "Epoch 39/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 3.5480 - accuracy: 0.1777\n",
            "Epoch 40/800\n",
            "59/59 [==============================] - 5s 87ms/step - loss: 3.4864 - accuracy: 0.1852\n",
            "Epoch 41/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 3.4350 - accuracy: 0.2013\n",
            "Epoch 42/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 3.3855 - accuracy: 0.2024\n",
            "Epoch 43/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 3.3292 - accuracy: 0.2190\n",
            "Epoch 44/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 3.2750 - accuracy: 0.2211\n",
            "Epoch 45/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 3.2216 - accuracy: 0.2291\n",
            "Epoch 46/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 3.1746 - accuracy: 0.2414\n",
            "Epoch 47/800\n",
            "59/59 [==============================] - 7s 115ms/step - loss: 3.1171 - accuracy: 0.2634\n",
            "Epoch 48/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 3.0643 - accuracy: 0.2752\n",
            "Epoch 49/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 3.0255 - accuracy: 0.2757\n",
            "Epoch 50/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 2.9738 - accuracy: 0.2960\n",
            "Epoch 51/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 2.9193 - accuracy: 0.3014\n",
            "Epoch 52/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 2.8611 - accuracy: 0.3249\n",
            "Epoch 53/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 2.8239 - accuracy: 0.3287\n",
            "Epoch 54/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 2.7733 - accuracy: 0.3330\n",
            "Epoch 55/800\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 2.7202 - accuracy: 0.3571\n",
            "Epoch 56/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 2.6846 - accuracy: 0.3512\n",
            "Epoch 57/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 2.6287 - accuracy: 0.3731\n",
            "Epoch 58/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 2.5767 - accuracy: 0.3887\n",
            "Epoch 59/800\n",
            "59/59 [==============================] - 5s 91ms/step - loss: 2.5308 - accuracy: 0.4010\n",
            "Epoch 60/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 2.4920 - accuracy: 0.4117\n",
            "Epoch 61/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 2.4411 - accuracy: 0.4245\n",
            "Epoch 62/800\n",
            "59/59 [==============================] - 5s 89ms/step - loss: 2.4011 - accuracy: 0.4384\n",
            "Epoch 63/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 2.3522 - accuracy: 0.4577\n",
            "Epoch 64/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 2.3211 - accuracy: 0.4593\n",
            "Epoch 65/800\n",
            "59/59 [==============================] - 5s 91ms/step - loss: 2.2656 - accuracy: 0.4807\n",
            "Epoch 66/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 2.2272 - accuracy: 0.4920\n",
            "Epoch 67/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 2.1860 - accuracy: 0.5091\n",
            "Epoch 68/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 2.1380 - accuracy: 0.5236\n",
            "Epoch 69/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 2.0999 - accuracy: 0.5380\n",
            "Epoch 70/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 2.0544 - accuracy: 0.5401\n",
            "Epoch 71/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 2.0199 - accuracy: 0.5444\n",
            "Epoch 72/800\n",
            "59/59 [==============================] - 6s 94ms/step - loss: 1.9751 - accuracy: 0.5632\n",
            "Epoch 73/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 1.9517 - accuracy: 0.5691\n",
            "Epoch 74/800\n",
            "59/59 [==============================] - 6s 102ms/step - loss: 1.9079 - accuracy: 0.5803\n",
            "Epoch 75/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 1.8747 - accuracy: 0.5958\n",
            "Epoch 76/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 1.8239 - accuracy: 0.6060\n",
            "Epoch 77/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 1.8030 - accuracy: 0.6081\n",
            "Epoch 78/800\n",
            "59/59 [==============================] - 6s 98ms/step - loss: 1.7631 - accuracy: 0.6247\n",
            "Epoch 79/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 1.7439 - accuracy: 0.6215\n",
            "Epoch 80/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 1.6976 - accuracy: 0.6397\n",
            "Epoch 81/800\n",
            "59/59 [==============================] - 6s 102ms/step - loss: 1.6670 - accuracy: 0.6483\n",
            "Epoch 82/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 1.6323 - accuracy: 0.6633\n",
            "Epoch 83/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 1.6038 - accuracy: 0.6654\n",
            "Epoch 84/800\n",
            "59/59 [==============================] - 6s 99ms/step - loss: 1.5799 - accuracy: 0.6681\n",
            "Epoch 85/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 1.5407 - accuracy: 0.6660\n",
            "Epoch 86/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 1.5027 - accuracy: 0.7002\n",
            "Epoch 87/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 1.4865 - accuracy: 0.6922\n",
            "Epoch 88/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 1.4554 - accuracy: 0.6890\n",
            "Epoch 89/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 1.4169 - accuracy: 0.7045\n",
            "Epoch 90/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 1.3936 - accuracy: 0.7152\n",
            "Epoch 91/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 1.3642 - accuracy: 0.7222\n",
            "Epoch 92/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 1.3357 - accuracy: 0.7206\n",
            "Epoch 93/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 1.3256 - accuracy: 0.7206\n",
            "Epoch 94/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 1.2755 - accuracy: 0.7382\n",
            "Epoch 95/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 1.2616 - accuracy: 0.7425\n",
            "Epoch 96/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 1.2403 - accuracy: 0.7382\n",
            "Epoch 97/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 1.2112 - accuracy: 0.7655\n",
            "Epoch 98/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 1.2012 - accuracy: 0.7639\n",
            "Epoch 99/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 1.1651 - accuracy: 0.7575\n",
            "Epoch 100/800\n",
            "59/59 [==============================] - 7s 116ms/step - loss: 1.1367 - accuracy: 0.7736\n",
            "Epoch 101/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 1.1218 - accuracy: 0.7784\n",
            "Epoch 102/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 1.1059 - accuracy: 0.7784\n",
            "Epoch 103/800\n",
            "59/59 [==============================] - 5s 92ms/step - loss: 1.0832 - accuracy: 0.7901\n",
            "Epoch 104/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 1.0568 - accuracy: 0.7939\n",
            "Epoch 105/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 1.0262 - accuracy: 0.7976\n",
            "Epoch 106/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 1.0217 - accuracy: 0.7939\n",
            "Epoch 107/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 0.9978 - accuracy: 0.8003\n",
            "Epoch 108/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.9724 - accuracy: 0.8003\n",
            "Epoch 109/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 0.9670 - accuracy: 0.8094\n",
            "Epoch 110/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.9450 - accuracy: 0.8180\n",
            "Epoch 111/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.9293 - accuracy: 0.8201\n",
            "Epoch 112/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.9109 - accuracy: 0.8244\n",
            "Epoch 113/800\n",
            "59/59 [==============================] - 5s 92ms/step - loss: 0.8782 - accuracy: 0.8239\n",
            "Epoch 114/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.8864 - accuracy: 0.8255\n",
            "Epoch 115/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.8593 - accuracy: 0.8362\n",
            "Epoch 116/800\n",
            "59/59 [==============================] - 5s 93ms/step - loss: 0.8369 - accuracy: 0.8362\n",
            "Epoch 117/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.8209 - accuracy: 0.8367\n",
            "Epoch 118/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.8050 - accuracy: 0.8415\n",
            "Epoch 119/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 0.7957 - accuracy: 0.8469\n",
            "Epoch 120/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.7822 - accuracy: 0.8399\n",
            "Epoch 121/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.7684 - accuracy: 0.8485\n",
            "Epoch 122/800\n",
            "59/59 [==============================] - 5s 94ms/step - loss: 0.7512 - accuracy: 0.8603\n",
            "Epoch 123/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.7363 - accuracy: 0.8630\n",
            "Epoch 124/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.7285 - accuracy: 0.8560\n",
            "Epoch 125/800\n",
            "59/59 [==============================] - 6s 104ms/step - loss: 0.7161 - accuracy: 0.8581\n",
            "Epoch 126/800\n",
            "59/59 [==============================] - 5s 76ms/step - loss: 0.7066 - accuracy: 0.8624\n",
            "Epoch 127/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.6927 - accuracy: 0.8688\n",
            "Epoch 128/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.6643 - accuracy: 0.8747\n",
            "Epoch 129/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.6679 - accuracy: 0.8753\n",
            "Epoch 130/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.6577 - accuracy: 0.8774\n",
            "Epoch 131/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.6302 - accuracy: 0.8769\n",
            "Epoch 132/800\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 0.6289 - accuracy: 0.8817\n",
            "Epoch 133/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.6196 - accuracy: 0.8838\n",
            "Epoch 134/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.6077 - accuracy: 0.8870\n",
            "Epoch 135/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.6050 - accuracy: 0.8838\n",
            "Epoch 136/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.5873 - accuracy: 0.8919\n",
            "Epoch 137/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.5818 - accuracy: 0.8881\n",
            "Epoch 138/800\n",
            "59/59 [==============================] - 5s 92ms/step - loss: 0.5647 - accuracy: 0.8892\n",
            "Epoch 139/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.5531 - accuracy: 0.8929\n",
            "Epoch 140/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.5439 - accuracy: 0.8919\n",
            "Epoch 141/800\n",
            "59/59 [==============================] - 6s 97ms/step - loss: 0.5389 - accuracy: 0.8924\n",
            "Epoch 142/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.5381 - accuracy: 0.8929\n",
            "Epoch 143/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.5328 - accuracy: 0.8988\n",
            "Epoch 144/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.5135 - accuracy: 0.9010\n",
            "Epoch 145/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.4924 - accuracy: 0.9090\n",
            "Epoch 146/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.5032 - accuracy: 0.9085\n",
            "Epoch 147/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.4733 - accuracy: 0.9069\n",
            "Epoch 148/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.4793 - accuracy: 0.9095\n",
            "Epoch 149/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.4582 - accuracy: 0.9149\n",
            "Epoch 150/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 0.4636 - accuracy: 0.9165\n",
            "Epoch 151/800\n",
            "59/59 [==============================] - 7s 112ms/step - loss: 0.4568 - accuracy: 0.9160\n",
            "Epoch 152/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.4430 - accuracy: 0.9149\n",
            "Epoch 153/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.4351 - accuracy: 0.9101\n",
            "Epoch 154/800\n",
            "59/59 [==============================] - 5s 87ms/step - loss: 0.4283 - accuracy: 0.9181\n",
            "Epoch 155/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.4196 - accuracy: 0.9240\n",
            "Epoch 156/800\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 0.4176 - accuracy: 0.9208\n",
            "Epoch 157/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 0.4255 - accuracy: 0.9170\n",
            "Epoch 158/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.4101 - accuracy: 0.9213\n",
            "Epoch 159/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.4101 - accuracy: 0.9208\n",
            "Epoch 160/800\n",
            "59/59 [==============================] - 5s 89ms/step - loss: 0.3953 - accuracy: 0.9186\n",
            "Epoch 161/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.3837 - accuracy: 0.9277\n",
            "Epoch 162/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.3764 - accuracy: 0.9234\n",
            "Epoch 163/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.3781 - accuracy: 0.9272\n",
            "Epoch 164/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.3609 - accuracy: 0.9320\n",
            "Epoch 165/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.3751 - accuracy: 0.9251\n",
            "Epoch 166/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.3639 - accuracy: 0.9267\n",
            "Epoch 167/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.3513 - accuracy: 0.9304\n",
            "Epoch 168/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.3502 - accuracy: 0.9288\n",
            "Epoch 169/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.3532 - accuracy: 0.9277\n",
            "Epoch 170/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.3324 - accuracy: 0.9379\n",
            "Epoch 171/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.3305 - accuracy: 0.9363\n",
            "Epoch 172/800\n",
            "59/59 [==============================] - 6s 98ms/step - loss: 0.3338 - accuracy: 0.9342\n",
            "Epoch 173/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.3242 - accuracy: 0.9374\n",
            "Epoch 174/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.3205 - accuracy: 0.9363\n",
            "Epoch 175/800\n",
            "59/59 [==============================] - 7s 112ms/step - loss: 0.3109 - accuracy: 0.9390\n",
            "Epoch 176/800\n",
            "59/59 [==============================] - 5s 76ms/step - loss: 0.3088 - accuracy: 0.9390\n",
            "Epoch 177/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.3051 - accuracy: 0.9384\n",
            "Epoch 178/800\n",
            "59/59 [==============================] - 5s 93ms/step - loss: 0.3013 - accuracy: 0.9454\n",
            "Epoch 179/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.3002 - accuracy: 0.9400\n",
            "Epoch 180/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.2846 - accuracy: 0.9465\n",
            "Epoch 181/800\n",
            "59/59 [==============================] - 5s 93ms/step - loss: 0.2927 - accuracy: 0.9384\n",
            "Epoch 182/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.2750 - accuracy: 0.9470\n",
            "Epoch 183/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.2740 - accuracy: 0.9449\n",
            "Epoch 184/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 0.2884 - accuracy: 0.9368\n",
            "Epoch 185/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.2662 - accuracy: 0.9507\n",
            "Epoch 186/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.2724 - accuracy: 0.9433\n",
            "Epoch 187/800\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 0.2766 - accuracy: 0.9433\n",
            "Epoch 188/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 0.2650 - accuracy: 0.9449\n",
            "Epoch 189/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.2707 - accuracy: 0.9395\n",
            "Epoch 190/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.2612 - accuracy: 0.9486\n",
            "Epoch 191/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.2548 - accuracy: 0.9433\n",
            "Epoch 192/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.2508 - accuracy: 0.9497\n",
            "Epoch 193/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 0.2396 - accuracy: 0.9470\n",
            "Epoch 194/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.2445 - accuracy: 0.9454\n",
            "Epoch 195/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.2390 - accuracy: 0.9507\n",
            "Epoch 196/800\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 0.2304 - accuracy: 0.9475\n",
            "Epoch 197/800\n",
            "59/59 [==============================] - 5s 89ms/step - loss: 0.2336 - accuracy: 0.9433\n",
            "Epoch 198/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.2249 - accuracy: 0.9518\n",
            "Epoch 199/800\n",
            "59/59 [==============================] - 5s 89ms/step - loss: 0.2287 - accuracy: 0.9524\n",
            "Epoch 200/800\n",
            "59/59 [==============================] - 7s 117ms/step - loss: 0.2216 - accuracy: 0.9491\n",
            "Epoch 201/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.2300 - accuracy: 0.9470\n",
            "Epoch 202/800\n",
            "59/59 [==============================] - 6s 104ms/step - loss: 0.2199 - accuracy: 0.9513\n",
            "Epoch 203/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.2179 - accuracy: 0.9502\n",
            "Epoch 204/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.2200 - accuracy: 0.9524\n",
            "Epoch 205/800\n",
            "59/59 [==============================] - 6s 99ms/step - loss: 0.2209 - accuracy: 0.9486\n",
            "Epoch 206/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.2122 - accuracy: 0.9534\n",
            "Epoch 207/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.2111 - accuracy: 0.9486\n",
            "Epoch 208/800\n",
            "59/59 [==============================] - 6s 94ms/step - loss: 0.2015 - accuracy: 0.9524\n",
            "Epoch 209/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.2059 - accuracy: 0.9502\n",
            "Epoch 210/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.2068 - accuracy: 0.9513\n",
            "Epoch 211/800\n",
            "59/59 [==============================] - 6s 98ms/step - loss: 0.1975 - accuracy: 0.9545\n",
            "Epoch 212/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.1878 - accuracy: 0.9556\n",
            "Epoch 213/800\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 0.1974 - accuracy: 0.9534\n",
            "Epoch 214/800\n",
            "59/59 [==============================] - 6s 98ms/step - loss: 0.1922 - accuracy: 0.9609\n",
            "Epoch 215/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.2024 - accuracy: 0.9524\n",
            "Epoch 216/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 0.1953 - accuracy: 0.9534\n",
            "Epoch 217/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.1893 - accuracy: 0.9540\n",
            "Epoch 218/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.1920 - accuracy: 0.9524\n",
            "Epoch 219/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 0.1802 - accuracy: 0.9540\n",
            "Epoch 220/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 0.1853 - accuracy: 0.9540\n",
            "Epoch 221/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.1800 - accuracy: 0.9540\n",
            "Epoch 222/800\n",
            "59/59 [==============================] - 5s 92ms/step - loss: 0.1742 - accuracy: 0.9572\n",
            "Epoch 223/800\n",
            "59/59 [==============================] - 7s 120ms/step - loss: 0.1741 - accuracy: 0.9566\n",
            "Epoch 224/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.1823 - accuracy: 0.9550\n",
            "Epoch 225/800\n",
            "59/59 [==============================] - 6s 100ms/step - loss: 0.1747 - accuracy: 0.9550\n",
            "Epoch 226/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.1732 - accuracy: 0.9577\n",
            "Epoch 227/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.1678 - accuracy: 0.9588\n",
            "Epoch 228/800\n",
            "59/59 [==============================] - 6s 103ms/step - loss: 0.1691 - accuracy: 0.9620\n",
            "Epoch 229/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.1644 - accuracy: 0.9636\n",
            "Epoch 230/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.1629 - accuracy: 0.9582\n",
            "Epoch 231/800\n",
            "59/59 [==============================] - 6s 100ms/step - loss: 0.1661 - accuracy: 0.9561\n",
            "Epoch 232/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.1565 - accuracy: 0.9599\n",
            "Epoch 233/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.1591 - accuracy: 0.9588\n",
            "Epoch 234/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.1700 - accuracy: 0.9556\n",
            "Epoch 235/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1647 - accuracy: 0.9529\n",
            "Epoch 236/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1524 - accuracy: 0.9599\n",
            "Epoch 237/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.1632 - accuracy: 0.9545\n",
            "Epoch 238/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1553 - accuracy: 0.9620\n",
            "Epoch 239/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.1530 - accuracy: 0.9588\n",
            "Epoch 240/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.1482 - accuracy: 0.9620\n",
            "Epoch 241/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1552 - accuracy: 0.9593\n",
            "Epoch 242/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.1516 - accuracy: 0.9593\n",
            "Epoch 243/800\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 0.1497 - accuracy: 0.9604\n",
            "Epoch 244/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.1454 - accuracy: 0.9577\n",
            "Epoch 245/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.1574 - accuracy: 0.9593\n",
            "Epoch 246/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.1570 - accuracy: 0.9566\n",
            "Epoch 247/800\n",
            "59/59 [==============================] - 7s 114ms/step - loss: 0.1466 - accuracy: 0.9588\n",
            "Epoch 248/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.1431 - accuracy: 0.9615\n",
            "Epoch 249/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.1366 - accuracy: 0.9599\n",
            "Epoch 250/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.1422 - accuracy: 0.9620\n",
            "Epoch 251/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.1473 - accuracy: 0.9582\n",
            "Epoch 252/800\n",
            "59/59 [==============================] - 5s 78ms/step - loss: 0.1425 - accuracy: 0.9577\n",
            "Epoch 253/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 0.1426 - accuracy: 0.9550\n",
            "Epoch 254/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.1337 - accuracy: 0.9620\n",
            "Epoch 255/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 0.1347 - accuracy: 0.9588\n",
            "Epoch 256/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.1333 - accuracy: 0.9609\n",
            "Epoch 257/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.1356 - accuracy: 0.9588\n",
            "Epoch 258/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.1328 - accuracy: 0.9599\n",
            "Epoch 259/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.1380 - accuracy: 0.9593\n",
            "Epoch 260/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1244 - accuracy: 0.9636\n",
            "Epoch 261/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.1342 - accuracy: 0.9582\n",
            "Epoch 262/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.1384 - accuracy: 0.9588\n",
            "Epoch 263/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1319 - accuracy: 0.9636\n",
            "Epoch 264/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.1230 - accuracy: 0.9604\n",
            "Epoch 265/800\n",
            "59/59 [==============================] - 5s 93ms/step - loss: 0.1266 - accuracy: 0.9582\n",
            "Epoch 266/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1317 - accuracy: 0.9609\n",
            "Epoch 267/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1335 - accuracy: 0.9588\n",
            "Epoch 268/800\n",
            "59/59 [==============================] - 6s 94ms/step - loss: 0.1289 - accuracy: 0.9604\n",
            "Epoch 269/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.1236 - accuracy: 0.9604\n",
            "Epoch 270/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1196 - accuracy: 0.9631\n",
            "Epoch 271/800\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 0.1206 - accuracy: 0.9599\n",
            "Epoch 272/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.1247 - accuracy: 0.9615\n",
            "Epoch 273/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.1244 - accuracy: 0.9652\n",
            "Epoch 274/800\n",
            "59/59 [==============================] - 5s 93ms/step - loss: 0.1200 - accuracy: 0.9599\n",
            "Epoch 275/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.1278 - accuracy: 0.9599\n",
            "Epoch 276/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.1207 - accuracy: 0.9636\n",
            "Epoch 277/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 0.1226 - accuracy: 0.9615\n",
            "Epoch 278/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.1221 - accuracy: 0.9599\n",
            "Epoch 279/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.1168 - accuracy: 0.9636\n",
            "Epoch 280/800\n",
            "59/59 [==============================] - 5s 87ms/step - loss: 0.1188 - accuracy: 0.9609\n",
            "Epoch 281/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.1182 - accuracy: 0.9609\n",
            "Epoch 282/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1176 - accuracy: 0.9593\n",
            "Epoch 283/800\n",
            "59/59 [==============================] - 4s 76ms/step - loss: 0.1232 - accuracy: 0.9593\n",
            "Epoch 284/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 0.1295 - accuracy: 0.9615\n",
            "Epoch 285/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1229 - accuracy: 0.9615\n",
            "Epoch 286/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.1230 - accuracy: 0.9609\n",
            "Epoch 287/800\n",
            "59/59 [==============================] - 5s 87ms/step - loss: 0.1223 - accuracy: 0.9631\n",
            "Epoch 288/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1248 - accuracy: 0.9593\n",
            "Epoch 289/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1251 - accuracy: 0.9636\n",
            "Epoch 290/800\n",
            "59/59 [==============================] - 6s 98ms/step - loss: 0.1152 - accuracy: 0.9652\n",
            "Epoch 291/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.1193 - accuracy: 0.9604\n",
            "Epoch 292/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.1159 - accuracy: 0.9636\n",
            "Epoch 293/800\n",
            "59/59 [==============================] - 6s 93ms/step - loss: 0.1120 - accuracy: 0.9615\n",
            "Epoch 294/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.1102 - accuracy: 0.9615\n",
            "Epoch 295/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.1120 - accuracy: 0.9588\n",
            "Epoch 296/800\n",
            "59/59 [==============================] - 6s 104ms/step - loss: 0.1105 - accuracy: 0.9615\n",
            "Epoch 297/800\n",
            "59/59 [==============================] - 6s 102ms/step - loss: 0.1091 - accuracy: 0.9663\n",
            "Epoch 298/800\n",
            "59/59 [==============================] - 5s 93ms/step - loss: 0.1039 - accuracy: 0.9652\n",
            "Epoch 299/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.1049 - accuracy: 0.9657\n",
            "Epoch 300/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.1049 - accuracy: 0.9647\n",
            "Epoch 301/800\n",
            "59/59 [==============================] - 5s 93ms/step - loss: 0.1093 - accuracy: 0.9615\n",
            "Epoch 302/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.1104 - accuracy: 0.9609\n",
            "Epoch 303/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.1030 - accuracy: 0.9636\n",
            "Epoch 304/800\n",
            "59/59 [==============================] - 6s 94ms/step - loss: 0.1098 - accuracy: 0.9620\n",
            "Epoch 305/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 0.1085 - accuracy: 0.9604\n",
            "Epoch 306/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.1103 - accuracy: 0.9625\n",
            "Epoch 307/800\n",
            "59/59 [==============================] - 5s 91ms/step - loss: 0.1122 - accuracy: 0.9631\n",
            "Epoch 308/800\n",
            "59/59 [==============================] - 5s 76ms/step - loss: 0.1096 - accuracy: 0.9593\n",
            "Epoch 309/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.1046 - accuracy: 0.9636\n",
            "Epoch 310/800\n",
            "59/59 [==============================] - 5s 89ms/step - loss: 0.0988 - accuracy: 0.9615\n",
            "Epoch 311/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.0992 - accuracy: 0.9620\n",
            "Epoch 312/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0973 - accuracy: 0.9647\n",
            "Epoch 313/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 0.0951 - accuracy: 0.9652\n",
            "Epoch 314/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 0.1007 - accuracy: 0.9636\n",
            "Epoch 315/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.1077 - accuracy: 0.9582\n",
            "Epoch 316/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 0.1059 - accuracy: 0.9641\n",
            "Epoch 317/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 0.0997 - accuracy: 0.9620\n",
            "Epoch 318/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0960 - accuracy: 0.9609\n",
            "Epoch 319/800\n",
            "59/59 [==============================] - 5s 84ms/step - loss: 0.1009 - accuracy: 0.9615\n",
            "Epoch 320/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 0.0993 - accuracy: 0.9631\n",
            "Epoch 321/800\n",
            "59/59 [==============================] - 6s 101ms/step - loss: 0.0920 - accuracy: 0.9673\n",
            "Epoch 322/800\n",
            "59/59 [==============================] - 6s 98ms/step - loss: 0.1001 - accuracy: 0.9636\n",
            "Epoch 323/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0971 - accuracy: 0.9641\n",
            "Epoch 324/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.1001 - accuracy: 0.9620\n",
            "Epoch 325/800\n",
            "59/59 [==============================] - 6s 102ms/step - loss: 0.0918 - accuracy: 0.9668\n",
            "Epoch 326/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0942 - accuracy: 0.9673\n",
            "Epoch 327/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0986 - accuracy: 0.9599\n",
            "Epoch 328/800\n",
            "59/59 [==============================] - 6s 99ms/step - loss: 0.0963 - accuracy: 0.9615\n",
            "Epoch 329/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.1006 - accuracy: 0.9625\n",
            "Epoch 330/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0970 - accuracy: 0.9625\n",
            "Epoch 331/800\n",
            "59/59 [==============================] - 6s 101ms/step - loss: 0.0986 - accuracy: 0.9625\n",
            "Epoch 332/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0976 - accuracy: 0.9609\n",
            "Epoch 333/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.1037 - accuracy: 0.9652\n",
            "Epoch 334/800\n",
            "59/59 [==============================] - 6s 102ms/step - loss: 0.0957 - accuracy: 0.9673\n",
            "Epoch 335/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0996 - accuracy: 0.9625\n",
            "Epoch 336/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0934 - accuracy: 0.9657\n",
            "Epoch 337/800\n",
            "59/59 [==============================] - 6s 103ms/step - loss: 0.0919 - accuracy: 0.9673\n",
            "Epoch 338/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0987 - accuracy: 0.9593\n",
            "Epoch 339/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0942 - accuracy: 0.9663\n",
            "Epoch 340/800\n",
            "59/59 [==============================] - 6s 104ms/step - loss: 0.0909 - accuracy: 0.9668\n",
            "Epoch 341/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0927 - accuracy: 0.9663\n",
            "Epoch 342/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0975 - accuracy: 0.9631\n",
            "Epoch 343/800\n",
            "59/59 [==============================] - 6s 98ms/step - loss: 0.0931 - accuracy: 0.9625\n",
            "Epoch 344/800\n",
            "59/59 [==============================] - 6s 103ms/step - loss: 0.0896 - accuracy: 0.9657\n",
            "Epoch 345/800\n",
            "59/59 [==============================] - 6s 97ms/step - loss: 0.1017 - accuracy: 0.9620\n",
            "Epoch 346/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0886 - accuracy: 0.9647\n",
            "Epoch 347/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0967 - accuracy: 0.9620\n",
            "Epoch 348/800\n",
            "59/59 [==============================] - 6s 103ms/step - loss: 0.0994 - accuracy: 0.9615\n",
            "Epoch 349/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0992 - accuracy: 0.9615\n",
            "Epoch 350/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0963 - accuracy: 0.9599\n",
            "Epoch 351/800\n",
            "59/59 [==============================] - 6s 103ms/step - loss: 0.0884 - accuracy: 0.9673\n",
            "Epoch 352/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0961 - accuracy: 0.9625\n",
            "Epoch 353/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0888 - accuracy: 0.9652\n",
            "Epoch 354/800\n",
            "59/59 [==============================] - 6s 105ms/step - loss: 0.0876 - accuracy: 0.9652\n",
            "Epoch 355/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0921 - accuracy: 0.9615\n",
            "Epoch 356/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0951 - accuracy: 0.9599\n",
            "Epoch 357/800\n",
            "59/59 [==============================] - 6s 104ms/step - loss: 0.0868 - accuracy: 0.9663\n",
            "Epoch 358/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0965 - accuracy: 0.9599\n",
            "Epoch 359/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0928 - accuracy: 0.9609\n",
            "Epoch 360/800\n",
            "59/59 [==============================] - 6s 99ms/step - loss: 0.0900 - accuracy: 0.9636\n",
            "Epoch 361/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0856 - accuracy: 0.9641\n",
            "Epoch 362/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0847 - accuracy: 0.9673\n",
            "Epoch 363/800\n",
            "59/59 [==============================] - 6s 103ms/step - loss: 0.0885 - accuracy: 0.9620\n",
            "Epoch 364/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0834 - accuracy: 0.9663\n",
            "Epoch 365/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0884 - accuracy: 0.9663\n",
            "Epoch 366/800\n",
            "59/59 [==============================] - 6s 106ms/step - loss: 0.0913 - accuracy: 0.9615\n",
            "Epoch 367/800\n",
            "59/59 [==============================] - 6s 99ms/step - loss: 0.0896 - accuracy: 0.9668\n",
            "Epoch 368/800\n",
            "59/59 [==============================] - 5s 87ms/step - loss: 0.0902 - accuracy: 0.9631\n",
            "Epoch 369/800\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 0.0844 - accuracy: 0.9663\n",
            "Epoch 370/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0894 - accuracy: 0.9636\n",
            "Epoch 371/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.0904 - accuracy: 0.9652\n",
            "Epoch 372/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 0.0863 - accuracy: 0.9706\n",
            "Epoch 373/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0884 - accuracy: 0.9636\n",
            "Epoch 374/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.0895 - accuracy: 0.9604\n",
            "Epoch 375/800\n",
            "59/59 [==============================] - 5s 75ms/step - loss: 0.1025 - accuracy: 0.9620\n",
            "Epoch 376/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0988 - accuracy: 0.9631\n",
            "Epoch 377/800\n",
            "59/59 [==============================] - 6s 100ms/step - loss: 0.0928 - accuracy: 0.9636\n",
            "Epoch 378/800\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 0.0907 - accuracy: 0.9636\n",
            "Epoch 379/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0864 - accuracy: 0.9673\n",
            "Epoch 380/800\n",
            "59/59 [==============================] - 6s 106ms/step - loss: 0.0856 - accuracy: 0.9641\n",
            "Epoch 381/800\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 0.0875 - accuracy: 0.9636\n",
            "Epoch 382/800\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 0.0857 - accuracy: 0.9636\n",
            "Epoch 383/800\n",
            "59/59 [==============================] - 6s 102ms/step - loss: 0.0847 - accuracy: 0.9641\n",
            "Epoch 384/800\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 0.0953 - accuracy: 0.9631\n",
            "Epoch 385/800\n",
            "59/59 [==============================] - 5s 87ms/step - loss: 0.0886 - accuracy: 0.9631\n",
            "Epoch 386/800\n",
            "59/59 [==============================] - 7s 114ms/step - loss: 0.0905 - accuracy: 0.9631\n",
            "Epoch 387/800\n",
            "59/59 [==============================] - 5s 84ms/step - loss: 0.0859 - accuracy: 0.9631\n",
            "Epoch 388/800\n",
            "59/59 [==============================] - 6s 99ms/step - loss: 0.0889 - accuracy: 0.9647\n",
            "Epoch 389/800\n",
            "59/59 [==============================] - 6s 107ms/step - loss: 0.0830 - accuracy: 0.9647\n",
            "Epoch 390/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.0745 - accuracy: 0.9695\n",
            "Epoch 391/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.0810 - accuracy: 0.9668\n",
            "Epoch 392/800\n",
            "59/59 [==============================] - 5s 76ms/step - loss: 0.0829 - accuracy: 0.9631\n",
            "Epoch 393/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0846 - accuracy: 0.9652\n",
            "Epoch 394/800\n",
            "59/59 [==============================] - 6s 97ms/step - loss: 0.0833 - accuracy: 0.9625\n",
            "Epoch 395/800\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 0.0844 - accuracy: 0.9625\n",
            "Epoch 396/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0801 - accuracy: 0.9647\n",
            "Epoch 397/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.0844 - accuracy: 0.9636\n",
            "Epoch 398/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0775 - accuracy: 0.9700\n",
            "Epoch 399/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0817 - accuracy: 0.9663\n",
            "Epoch 400/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.0923 - accuracy: 0.9609\n",
            "Epoch 401/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0846 - accuracy: 0.9620\n",
            "Epoch 402/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0820 - accuracy: 0.9690\n",
            "Epoch 403/800\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 0.0867 - accuracy: 0.9631\n",
            "Epoch 404/800\n",
            "59/59 [==============================] - 5s 79ms/step - loss: 0.0797 - accuracy: 0.9657\n",
            "Epoch 405/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0828 - accuracy: 0.9647\n",
            "Epoch 406/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0935 - accuracy: 0.9636\n",
            "Epoch 407/800\n",
            "59/59 [==============================] - 6s 94ms/step - loss: 0.1405 - accuracy: 0.9481\n",
            "Epoch 408/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.1023 - accuracy: 0.9615\n",
            "Epoch 409/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0923 - accuracy: 0.9636\n",
            "Epoch 410/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 0.0819 - accuracy: 0.9668\n",
            "Epoch 411/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0848 - accuracy: 0.9673\n",
            "Epoch 412/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0783 - accuracy: 0.9684\n",
            "Epoch 413/800\n",
            "59/59 [==============================] - 6s 102ms/step - loss: 0.0828 - accuracy: 0.9684\n",
            "Epoch 414/800\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 0.0828 - accuracy: 0.9668\n",
            "Epoch 415/800\n",
            "59/59 [==============================] - 3s 59ms/step - loss: 0.0845 - accuracy: 0.9599\n",
            "Epoch 416/800\n",
            "59/59 [==============================] - 3s 59ms/step - loss: 0.0781 - accuracy: 0.9690\n",
            "Epoch 417/800\n",
            "59/59 [==============================] - 5s 89ms/step - loss: 0.0825 - accuracy: 0.9636\n",
            "Epoch 418/800\n",
            "59/59 [==============================] - 3s 58ms/step - loss: 0.0770 - accuracy: 0.9695\n",
            "Epoch 419/800\n",
            "59/59 [==============================] - 3s 59ms/step - loss: 0.0803 - accuracy: 0.9700\n",
            "Epoch 420/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 0.0824 - accuracy: 0.9647\n",
            "Epoch 421/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0766 - accuracy: 0.9684\n",
            "Epoch 422/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0844 - accuracy: 0.9636\n",
            "Epoch 423/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.0801 - accuracy: 0.9652\n",
            "Epoch 424/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.0861 - accuracy: 0.9673\n",
            "Epoch 425/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0852 - accuracy: 0.9636\n",
            "Epoch 426/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0840 - accuracy: 0.9657\n",
            "Epoch 427/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.0792 - accuracy: 0.9641\n",
            "Epoch 428/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0841 - accuracy: 0.9668\n",
            "Epoch 429/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0836 - accuracy: 0.9652\n",
            "Epoch 430/800\n",
            "59/59 [==============================] - 5s 87ms/step - loss: 0.0798 - accuracy: 0.9657\n",
            "Epoch 431/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0833 - accuracy: 0.9647\n",
            "Epoch 432/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0838 - accuracy: 0.9652\n",
            "Epoch 433/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0758 - accuracy: 0.9668\n",
            "Epoch 434/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 0.0784 - accuracy: 0.9679\n",
            "Epoch 435/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0773 - accuracy: 0.9684\n",
            "Epoch 436/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0805 - accuracy: 0.9636\n",
            "Epoch 437/800\n",
            "59/59 [==============================] - 5s 84ms/step - loss: 0.0774 - accuracy: 0.9684\n",
            "Epoch 438/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0825 - accuracy: 0.9641\n",
            "Epoch 439/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0767 - accuracy: 0.9641\n",
            "Epoch 440/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.0823 - accuracy: 0.9641\n",
            "Epoch 441/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0760 - accuracy: 0.9679\n",
            "Epoch 442/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0786 - accuracy: 0.9620\n",
            "Epoch 443/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.0753 - accuracy: 0.9673\n",
            "Epoch 444/800\n",
            "59/59 [==============================] - 5s 78ms/step - loss: 0.0782 - accuracy: 0.9673\n",
            "Epoch 445/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0849 - accuracy: 0.9641\n",
            "Epoch 446/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0823 - accuracy: 0.9641\n",
            "Epoch 447/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 0.0813 - accuracy: 0.9615\n",
            "Epoch 448/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0832 - accuracy: 0.9641\n",
            "Epoch 449/800\n",
            "59/59 [==============================] - 3s 59ms/step - loss: 0.0778 - accuracy: 0.9641\n",
            "Epoch 450/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.1091 - accuracy: 0.9572\n",
            "Epoch 451/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.1044 - accuracy: 0.9604\n",
            "Epoch 452/800\n",
            "59/59 [==============================] - 3s 59ms/step - loss: 0.0792 - accuracy: 0.9652\n",
            "Epoch 453/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0853 - accuracy: 0.9631\n",
            "Epoch 454/800\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 0.0828 - accuracy: 0.9615\n",
            "Epoch 455/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0752 - accuracy: 0.9690\n",
            "Epoch 456/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0863 - accuracy: 0.9599\n",
            "Epoch 457/800\n",
            "59/59 [==============================] - 5s 89ms/step - loss: 0.0813 - accuracy: 0.9631\n",
            "Epoch 458/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0772 - accuracy: 0.9657\n",
            "Epoch 459/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0790 - accuracy: 0.9647\n",
            "Epoch 460/800\n",
            "59/59 [==============================] - 6s 94ms/step - loss: 0.0771 - accuracy: 0.9684\n",
            "Epoch 461/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0784 - accuracy: 0.9668\n",
            "Epoch 462/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0843 - accuracy: 0.9652\n",
            "Epoch 463/800\n",
            "59/59 [==============================] - 5s 87ms/step - loss: 0.0751 - accuracy: 0.9663\n",
            "Epoch 464/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0839 - accuracy: 0.9641\n",
            "Epoch 465/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0773 - accuracy: 0.9690\n",
            "Epoch 466/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.0832 - accuracy: 0.9636\n",
            "Epoch 467/800\n",
            "59/59 [==============================] - 5s 79ms/step - loss: 0.0792 - accuracy: 0.9641\n",
            "Epoch 468/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0844 - accuracy: 0.9636\n",
            "Epoch 469/800\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 0.0811 - accuracy: 0.9636\n",
            "Epoch 470/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0831 - accuracy: 0.9620\n",
            "Epoch 471/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0791 - accuracy: 0.9652\n",
            "Epoch 472/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 0.0784 - accuracy: 0.9657\n",
            "Epoch 473/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0811 - accuracy: 0.9625\n",
            "Epoch 474/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0782 - accuracy: 0.9641\n",
            "Epoch 475/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0760 - accuracy: 0.9684\n",
            "Epoch 476/800\n",
            "59/59 [==============================] - 5s 76ms/step - loss: 0.0796 - accuracy: 0.9641\n",
            "Epoch 477/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0803 - accuracy: 0.9663\n",
            "Epoch 478/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0783 - accuracy: 0.9663\n",
            "Epoch 479/800\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 0.0746 - accuracy: 0.9668\n",
            "Epoch 480/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0790 - accuracy: 0.9663\n",
            "Epoch 481/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0749 - accuracy: 0.9652\n",
            "Epoch 482/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.0770 - accuracy: 0.9652\n",
            "Epoch 483/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0766 - accuracy: 0.9668\n",
            "Epoch 484/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0779 - accuracy: 0.9657\n",
            "Epoch 485/800\n",
            "59/59 [==============================] - 5s 84ms/step - loss: 0.0776 - accuracy: 0.9668\n",
            "Epoch 486/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0770 - accuracy: 0.9647\n",
            "Epoch 487/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0717 - accuracy: 0.9679\n",
            "Epoch 488/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.0807 - accuracy: 0.9652\n",
            "Epoch 489/800\n",
            "59/59 [==============================] - 5s 78ms/step - loss: 0.0778 - accuracy: 0.9636\n",
            "Epoch 490/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0829 - accuracy: 0.9641\n",
            "Epoch 491/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.1076 - accuracy: 0.9604\n",
            "Epoch 492/800\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 0.1012 - accuracy: 0.9577\n",
            "Epoch 493/800\n",
            "59/59 [==============================] - 5s 84ms/step - loss: 0.0916 - accuracy: 0.9620\n",
            "Epoch 494/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.0898 - accuracy: 0.9631\n",
            "Epoch 495/800\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 0.0846 - accuracy: 0.9647\n",
            "Epoch 496/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0839 - accuracy: 0.9625\n",
            "Epoch 497/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0782 - accuracy: 0.9641\n",
            "Epoch 498/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.0854 - accuracy: 0.9604\n",
            "Epoch 499/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0811 - accuracy: 0.9636\n",
            "Epoch 500/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0795 - accuracy: 0.9647\n",
            "Epoch 501/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.0750 - accuracy: 0.9652\n",
            "Epoch 502/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0730 - accuracy: 0.9652\n",
            "Epoch 503/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0757 - accuracy: 0.9690\n",
            "Epoch 504/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.0803 - accuracy: 0.9636\n",
            "Epoch 505/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0811 - accuracy: 0.9636\n",
            "Epoch 506/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0784 - accuracy: 0.9636\n",
            "Epoch 507/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 0.0792 - accuracy: 0.9636\n",
            "Epoch 508/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0710 - accuracy: 0.9716\n",
            "Epoch 509/800\n",
            "59/59 [==============================] - 3s 58ms/step - loss: 0.0751 - accuracy: 0.9673\n",
            "Epoch 510/800\n",
            "59/59 [==============================] - 3s 58ms/step - loss: 0.0807 - accuracy: 0.9625\n",
            "Epoch 511/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 0.0801 - accuracy: 0.9636\n",
            "Epoch 512/800\n",
            "59/59 [==============================] - 3s 59ms/step - loss: 0.0715 - accuracy: 0.9700\n",
            "Epoch 513/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0777 - accuracy: 0.9652\n",
            "Epoch 514/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 0.0782 - accuracy: 0.9663\n",
            "Epoch 515/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0810 - accuracy: 0.9604\n",
            "Epoch 516/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0755 - accuracy: 0.9673\n",
            "Epoch 517/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.0777 - accuracy: 0.9615\n",
            "Epoch 518/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0820 - accuracy: 0.9593\n",
            "Epoch 519/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0819 - accuracy: 0.9647\n",
            "Epoch 520/800\n",
            "59/59 [==============================] - 6s 104ms/step - loss: 0.0754 - accuracy: 0.9663\n",
            "Epoch 521/800\n",
            "59/59 [==============================] - 5s 76ms/step - loss: 0.0768 - accuracy: 0.9647\n",
            "Epoch 522/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0774 - accuracy: 0.9631\n",
            "Epoch 523/800\n",
            "59/59 [==============================] - 5s 84ms/step - loss: 0.0746 - accuracy: 0.9652\n",
            "Epoch 524/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 0.0776 - accuracy: 0.9652\n",
            "Epoch 525/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0798 - accuracy: 0.9663\n",
            "Epoch 526/800\n",
            "59/59 [==============================] - 5s 79ms/step - loss: 0.0753 - accuracy: 0.9657\n",
            "Epoch 527/800\n",
            "59/59 [==============================] - 5s 84ms/step - loss: 0.0760 - accuracy: 0.9652\n",
            "Epoch 528/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0775 - accuracy: 0.9647\n",
            "Epoch 529/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0768 - accuracy: 0.9657\n",
            "Epoch 530/800\n",
            "59/59 [==============================] - 5s 91ms/step - loss: 0.0769 - accuracy: 0.9647\n",
            "Epoch 531/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0736 - accuracy: 0.9663\n",
            "Epoch 532/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0773 - accuracy: 0.9641\n",
            "Epoch 533/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.0784 - accuracy: 0.9679\n",
            "Epoch 534/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0774 - accuracy: 0.9657\n",
            "Epoch 535/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0769 - accuracy: 0.9679\n",
            "Epoch 536/800\n",
            "59/59 [==============================] - 5s 92ms/step - loss: 0.0731 - accuracy: 0.9690\n",
            "Epoch 537/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.0800 - accuracy: 0.9625\n",
            "Epoch 538/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.0788 - accuracy: 0.9647\n",
            "Epoch 539/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.0751 - accuracy: 0.9695\n",
            "Epoch 540/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.0845 - accuracy: 0.9593\n",
            "Epoch 541/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0909 - accuracy: 0.9615\n",
            "Epoch 542/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.0816 - accuracy: 0.9652\n",
            "Epoch 543/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0744 - accuracy: 0.9647\n",
            "Epoch 544/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 0.0767 - accuracy: 0.9657\n",
            "Epoch 545/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.0783 - accuracy: 0.9615\n",
            "Epoch 546/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0779 - accuracy: 0.9615\n",
            "Epoch 547/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0768 - accuracy: 0.9631\n",
            "Epoch 548/800\n",
            "59/59 [==============================] - 5s 89ms/step - loss: 0.0783 - accuracy: 0.9679\n",
            "Epoch 549/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0844 - accuracy: 0.9604\n",
            "Epoch 550/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0748 - accuracy: 0.9657\n",
            "Epoch 551/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 0.0786 - accuracy: 0.9641\n",
            "Epoch 552/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0751 - accuracy: 0.9647\n",
            "Epoch 553/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0751 - accuracy: 0.9620\n",
            "Epoch 554/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 0.0757 - accuracy: 0.9668\n",
            "Epoch 555/800\n",
            "59/59 [==============================] - 5s 78ms/step - loss: 0.0771 - accuracy: 0.9684\n",
            "Epoch 556/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0742 - accuracy: 0.9663\n",
            "Epoch 557/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0710 - accuracy: 0.9684\n",
            "Epoch 558/800\n",
            "59/59 [==============================] - 5s 89ms/step - loss: 0.0729 - accuracy: 0.9657\n",
            "Epoch 559/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0768 - accuracy: 0.9647\n",
            "Epoch 560/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0738 - accuracy: 0.9663\n",
            "Epoch 561/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.0749 - accuracy: 0.9668\n",
            "Epoch 562/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0774 - accuracy: 0.9647\n",
            "Epoch 563/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0752 - accuracy: 0.9609\n",
            "Epoch 564/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 0.0753 - accuracy: 0.9641\n",
            "Epoch 565/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0775 - accuracy: 0.9647\n",
            "Epoch 566/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0778 - accuracy: 0.9657\n",
            "Epoch 567/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0736 - accuracy: 0.9700\n",
            "Epoch 568/800\n",
            "59/59 [==============================] - 5s 78ms/step - loss: 0.0760 - accuracy: 0.9647\n",
            "Epoch 569/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.0740 - accuracy: 0.9668\n",
            "Epoch 570/800\n",
            "59/59 [==============================] - 6s 103ms/step - loss: 0.0713 - accuracy: 0.9647\n",
            "Epoch 571/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 0.0721 - accuracy: 0.9679\n",
            "Epoch 572/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0740 - accuracy: 0.9641\n",
            "Epoch 573/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.0754 - accuracy: 0.9641\n",
            "Epoch 574/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.0725 - accuracy: 0.9657\n",
            "Epoch 575/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0734 - accuracy: 0.9652\n",
            "Epoch 576/800\n",
            "59/59 [==============================] - 5s 78ms/step - loss: 0.0761 - accuracy: 0.9631\n",
            "Epoch 577/800\n",
            "59/59 [==============================] - 5s 78ms/step - loss: 0.0748 - accuracy: 0.9636\n",
            "Epoch 578/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0728 - accuracy: 0.9668\n",
            "Epoch 579/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0727 - accuracy: 0.9652\n",
            "Epoch 580/800\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 0.0718 - accuracy: 0.9647\n",
            "Epoch 581/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0748 - accuracy: 0.9647\n",
            "Epoch 582/800\n",
            "59/59 [==============================] - 3s 59ms/step - loss: 0.0804 - accuracy: 0.9609\n",
            "Epoch 583/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.0884 - accuracy: 0.9641\n",
            "Epoch 584/800\n",
            "59/59 [==============================] - 3s 58ms/step - loss: 0.0944 - accuracy: 0.9615\n",
            "Epoch 585/800\n",
            "59/59 [==============================] - 3s 58ms/step - loss: 0.0832 - accuracy: 0.9668\n",
            "Epoch 586/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.0779 - accuracy: 0.9636\n",
            "Epoch 587/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0805 - accuracy: 0.9609\n",
            "Epoch 588/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0714 - accuracy: 0.9706\n",
            "Epoch 589/800\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 0.0726 - accuracy: 0.9668\n",
            "Epoch 590/800\n",
            "59/59 [==============================] - 5s 78ms/step - loss: 0.0742 - accuracy: 0.9625\n",
            "Epoch 591/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0823 - accuracy: 0.9620\n",
            "Epoch 592/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0742 - accuracy: 0.9620\n",
            "Epoch 593/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 0.0689 - accuracy: 0.9695\n",
            "Epoch 594/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0702 - accuracy: 0.9673\n",
            "Epoch 595/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0772 - accuracy: 0.9647\n",
            "Epoch 596/800\n",
            "59/59 [==============================] - 6s 103ms/step - loss: 0.0741 - accuracy: 0.9652\n",
            "Epoch 597/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0736 - accuracy: 0.9679\n",
            "Epoch 598/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0755 - accuracy: 0.9615\n",
            "Epoch 599/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.0721 - accuracy: 0.9647\n",
            "Epoch 600/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0757 - accuracy: 0.9636\n",
            "Epoch 601/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0709 - accuracy: 0.9684\n",
            "Epoch 602/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.0738 - accuracy: 0.9668\n",
            "Epoch 603/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 0.0716 - accuracy: 0.9657\n",
            "Epoch 604/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0692 - accuracy: 0.9679\n",
            "Epoch 605/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0740 - accuracy: 0.9615\n",
            "Epoch 606/800\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 0.0746 - accuracy: 0.9668\n",
            "Epoch 607/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0671 - accuracy: 0.9700\n",
            "Epoch 608/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0740 - accuracy: 0.9631\n",
            "Epoch 609/800\n",
            "59/59 [==============================] - 5s 91ms/step - loss: 0.0744 - accuracy: 0.9652\n",
            "Epoch 610/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0759 - accuracy: 0.9679\n",
            "Epoch 611/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0741 - accuracy: 0.9636\n",
            "Epoch 612/800\n",
            "59/59 [==============================] - 5s 92ms/step - loss: 0.0673 - accuracy: 0.9684\n",
            "Epoch 613/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0704 - accuracy: 0.9679\n",
            "Epoch 614/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0728 - accuracy: 0.9631\n",
            "Epoch 615/800\n",
            "59/59 [==============================] - 5s 79ms/step - loss: 0.0718 - accuracy: 0.9641\n",
            "Epoch 616/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0688 - accuracy: 0.9695\n",
            "Epoch 617/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0744 - accuracy: 0.9641\n",
            "Epoch 618/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0732 - accuracy: 0.9652\n",
            "Epoch 619/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.0731 - accuracy: 0.9647\n",
            "Epoch 620/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0760 - accuracy: 0.9652\n",
            "Epoch 621/800\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 0.0726 - accuracy: 0.9652\n",
            "Epoch 622/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.0744 - accuracy: 0.9641\n",
            "Epoch 623/800\n",
            "59/59 [==============================] - 3s 59ms/step - loss: 0.0722 - accuracy: 0.9657\n",
            "Epoch 624/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0823 - accuracy: 0.9620\n",
            "Epoch 625/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.1041 - accuracy: 0.9545\n",
            "Epoch 626/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0991 - accuracy: 0.9582\n",
            "Epoch 627/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0868 - accuracy: 0.9609\n",
            "Epoch 628/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.0809 - accuracy: 0.9609\n",
            "Epoch 629/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.0793 - accuracy: 0.9625\n",
            "Epoch 630/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0755 - accuracy: 0.9641\n",
            "Epoch 631/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.0769 - accuracy: 0.9615\n",
            "Epoch 632/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0754 - accuracy: 0.9652\n",
            "Epoch 633/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0709 - accuracy: 0.9690\n",
            "Epoch 634/800\n",
            "59/59 [==============================] - 5s 84ms/step - loss: 0.0755 - accuracy: 0.9636\n",
            "Epoch 635/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0748 - accuracy: 0.9663\n",
            "Epoch 636/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0753 - accuracy: 0.9668\n",
            "Epoch 637/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.0711 - accuracy: 0.9679\n",
            "Epoch 638/800\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 0.0719 - accuracy: 0.9636\n",
            "Epoch 639/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0694 - accuracy: 0.9684\n",
            "Epoch 640/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0742 - accuracy: 0.9636\n",
            "Epoch 641/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.0720 - accuracy: 0.9631\n",
            "Epoch 642/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0797 - accuracy: 0.9625\n",
            "Epoch 643/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0720 - accuracy: 0.9668\n",
            "Epoch 644/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.0736 - accuracy: 0.9647\n",
            "Epoch 645/800\n",
            "59/59 [==============================] - 4s 61ms/step - loss: 0.0757 - accuracy: 0.9631\n",
            "Epoch 646/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 0.0763 - accuracy: 0.9673\n",
            "Epoch 647/800\n",
            "59/59 [==============================] - 6s 94ms/step - loss: 0.0735 - accuracy: 0.9652\n",
            "Epoch 648/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0722 - accuracy: 0.9647\n",
            "Epoch 649/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0730 - accuracy: 0.9657\n",
            "Epoch 650/800\n",
            "59/59 [==============================] - 5s 87ms/step - loss: 0.0706 - accuracy: 0.9673\n",
            "Epoch 651/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0770 - accuracy: 0.9636\n",
            "Epoch 652/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0709 - accuracy: 0.9673\n",
            "Epoch 653/800\n",
            "59/59 [==============================] - 5s 91ms/step - loss: 0.0706 - accuracy: 0.9641\n",
            "Epoch 654/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0732 - accuracy: 0.9684\n",
            "Epoch 655/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0701 - accuracy: 0.9684\n",
            "Epoch 656/800\n",
            "59/59 [==============================] - 5s 93ms/step - loss: 0.0689 - accuracy: 0.9663\n",
            "Epoch 657/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0726 - accuracy: 0.9647\n",
            "Epoch 658/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0700 - accuracy: 0.9690\n",
            "Epoch 659/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 0.0721 - accuracy: 0.9652\n",
            "Epoch 660/800\n",
            "59/59 [==============================] - 4s 60ms/step - loss: 0.0705 - accuracy: 0.9663\n",
            "Epoch 661/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0737 - accuracy: 0.9625\n",
            "Epoch 662/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 0.0730 - accuracy: 0.9663\n",
            "Epoch 663/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0723 - accuracy: 0.9657\n",
            "Epoch 664/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0743 - accuracy: 0.9657\n",
            "Epoch 665/800\n",
            "59/59 [==============================] - 5s 79ms/step - loss: 0.0711 - accuracy: 0.9625\n",
            "Epoch 666/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.0719 - accuracy: 0.9647\n",
            "Epoch 667/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0725 - accuracy: 0.9652\n",
            "Epoch 668/800\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 0.0697 - accuracy: 0.9641\n",
            "Epoch 669/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 0.0693 - accuracy: 0.9684\n",
            "Epoch 670/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0693 - accuracy: 0.9690\n",
            "Epoch 671/800\n",
            "59/59 [==============================] - 7s 118ms/step - loss: 0.0711 - accuracy: 0.9636\n",
            "Epoch 672/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0719 - accuracy: 0.9625\n",
            "Epoch 673/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0693 - accuracy: 0.9657\n",
            "Epoch 674/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 0.0693 - accuracy: 0.9668\n",
            "Epoch 675/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0699 - accuracy: 0.9641\n",
            "Epoch 676/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0705 - accuracy: 0.9647\n",
            "Epoch 677/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 0.0759 - accuracy: 0.9620\n",
            "Epoch 678/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.0708 - accuracy: 0.9636\n",
            "Epoch 679/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0796 - accuracy: 0.9641\n",
            "Epoch 680/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 0.0853 - accuracy: 0.9615\n",
            "Epoch 681/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.0876 - accuracy: 0.9620\n",
            "Epoch 682/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0830 - accuracy: 0.9599\n",
            "Epoch 683/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 0.0753 - accuracy: 0.9641\n",
            "Epoch 684/800\n",
            "59/59 [==============================] - 5s 78ms/step - loss: 0.0707 - accuracy: 0.9652\n",
            "Epoch 685/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.0700 - accuracy: 0.9668\n",
            "Epoch 686/800\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 0.0708 - accuracy: 0.9647\n",
            "Epoch 687/800\n",
            "59/59 [==============================] - 5s 76ms/step - loss: 0.0742 - accuracy: 0.9652\n",
            "Epoch 688/800\n",
            "59/59 [==============================] - 4s 65ms/step - loss: 0.0693 - accuracy: 0.9684\n",
            "Epoch 689/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0740 - accuracy: 0.9625\n",
            "Epoch 690/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 0.0722 - accuracy: 0.9620\n",
            "Epoch 691/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0734 - accuracy: 0.9668\n",
            "Epoch 692/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0692 - accuracy: 0.9684\n",
            "Epoch 693/800\n",
            "59/59 [==============================] - 5s 92ms/step - loss: 0.0689 - accuracy: 0.9663\n",
            "Epoch 694/800\n",
            "59/59 [==============================] - 4s 64ms/step - loss: 0.0714 - accuracy: 0.9636\n",
            "Epoch 695/800\n",
            "59/59 [==============================] - 6s 102ms/step - loss: 0.0728 - accuracy: 0.9631\n",
            "Epoch 696/800\n",
            "59/59 [==============================] - 5s 87ms/step - loss: 0.0705 - accuracy: 0.9636\n",
            "Epoch 697/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0720 - accuracy: 0.9673\n",
            "Epoch 698/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.0724 - accuracy: 0.9636\n",
            "Epoch 699/800\n",
            "59/59 [==============================] - 5s 84ms/step - loss: 0.0697 - accuracy: 0.9652\n",
            "Epoch 700/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.0722 - accuracy: 0.9652\n",
            "Epoch 701/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 0.0756 - accuracy: 0.9609\n",
            "Epoch 702/800\n",
            "59/59 [==============================] - 5s 85ms/step - loss: 0.0726 - accuracy: 0.9657\n",
            "Epoch 703/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0674 - accuracy: 0.9657\n",
            "Epoch 704/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0729 - accuracy: 0.9620\n",
            "Epoch 705/800\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 0.0704 - accuracy: 0.9641\n",
            "Epoch 706/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0700 - accuracy: 0.9679\n",
            "Epoch 707/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0746 - accuracy: 0.9641\n",
            "Epoch 708/800\n",
            "59/59 [==============================] - 6s 94ms/step - loss: 0.0658 - accuracy: 0.9673\n",
            "Epoch 709/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0682 - accuracy: 0.9690\n",
            "Epoch 710/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0699 - accuracy: 0.9641\n",
            "Epoch 711/800\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 0.0686 - accuracy: 0.9668\n",
            "Epoch 712/800\n",
            "59/59 [==============================] - 4s 62ms/step - loss: 0.0652 - accuracy: 0.9690\n",
            "Epoch 713/800\n",
            "59/59 [==============================] - 4s 63ms/step - loss: 0.0681 - accuracy: 0.9679\n",
            "Epoch 714/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 0.0754 - accuracy: 0.9663\n",
            "Epoch 715/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0685 - accuracy: 0.9652\n",
            "Epoch 716/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0736 - accuracy: 0.9609\n",
            "Epoch 717/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.0756 - accuracy: 0.9631\n",
            "Epoch 718/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0729 - accuracy: 0.9652\n",
            "Epoch 719/800\n",
            "59/59 [==============================] - 5s 92ms/step - loss: 0.0765 - accuracy: 0.9615\n",
            "Epoch 720/800\n",
            "59/59 [==============================] - 6s 99ms/step - loss: 0.0751 - accuracy: 0.9609\n",
            "Epoch 721/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0736 - accuracy: 0.9625\n",
            "Epoch 722/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0694 - accuracy: 0.9679\n",
            "Epoch 723/800\n",
            "59/59 [==============================] - 6s 94ms/step - loss: 0.0709 - accuracy: 0.9641\n",
            "Epoch 724/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0700 - accuracy: 0.9668\n",
            "Epoch 725/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0736 - accuracy: 0.9641\n",
            "Epoch 726/800\n",
            "59/59 [==============================] - 5s 91ms/step - loss: 0.0673 - accuracy: 0.9679\n",
            "Epoch 727/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.0744 - accuracy: 0.9615\n",
            "Epoch 728/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0684 - accuracy: 0.9663\n",
            "Epoch 729/800\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 0.0737 - accuracy: 0.9657\n",
            "Epoch 730/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0758 - accuracy: 0.9668\n",
            "Epoch 731/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0763 - accuracy: 0.9620\n",
            "Epoch 732/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.0722 - accuracy: 0.9631\n",
            "Epoch 733/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0710 - accuracy: 0.9668\n",
            "Epoch 734/800\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 0.0728 - accuracy: 0.9679\n",
            "Epoch 735/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 0.0747 - accuracy: 0.9631\n",
            "Epoch 736/800\n",
            "59/59 [==============================] - 4s 67ms/step - loss: 0.0728 - accuracy: 0.9636\n",
            "Epoch 737/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0681 - accuracy: 0.9679\n",
            "Epoch 738/800\n",
            "59/59 [==============================] - 5s 92ms/step - loss: 0.0670 - accuracy: 0.9690\n",
            "Epoch 739/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0715 - accuracy: 0.9631\n",
            "Epoch 740/800\n",
            "59/59 [==============================] - 4s 68ms/step - loss: 0.0720 - accuracy: 0.9652\n",
            "Epoch 741/800\n",
            "59/59 [==============================] - 5s 92ms/step - loss: 0.0710 - accuracy: 0.9668\n",
            "Epoch 742/800\n",
            "59/59 [==============================] - 5s 91ms/step - loss: 0.0719 - accuracy: 0.9647\n",
            "Epoch 743/800\n",
            "59/59 [==============================] - 5s 92ms/step - loss: 0.0720 - accuracy: 0.9641\n",
            "Epoch 744/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0724 - accuracy: 0.9631\n",
            "Epoch 745/800\n",
            "59/59 [==============================] - 4s 66ms/step - loss: 0.0717 - accuracy: 0.9636\n",
            "Epoch 746/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 0.0693 - accuracy: 0.9641\n",
            "Epoch 747/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.0693 - accuracy: 0.9668\n",
            "Epoch 748/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0700 - accuracy: 0.9668\n",
            "Epoch 749/800\n",
            "59/59 [==============================] - 6s 99ms/step - loss: 0.0676 - accuracy: 0.9663\n",
            "Epoch 750/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0721 - accuracy: 0.9636\n",
            "Epoch 751/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0657 - accuracy: 0.9695\n",
            "Epoch 752/800\n",
            "59/59 [==============================] - 6s 101ms/step - loss: 0.0672 - accuracy: 0.9652\n",
            "Epoch 753/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0689 - accuracy: 0.9663\n",
            "Epoch 754/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.0716 - accuracy: 0.9657\n",
            "Epoch 755/800\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.0718 - accuracy: 0.9647\n",
            "Epoch 756/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0775 - accuracy: 0.9647\n",
            "Epoch 757/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0774 - accuracy: 0.9636\n",
            "Epoch 758/800\n",
            "59/59 [==============================] - 6s 101ms/step - loss: 0.1028 - accuracy: 0.9550\n",
            "Epoch 759/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0792 - accuracy: 0.9652\n",
            "Epoch 760/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.0771 - accuracy: 0.9631\n",
            "Epoch 761/800\n",
            "59/59 [==============================] - 6s 97ms/step - loss: 0.0703 - accuracy: 0.9663\n",
            "Epoch 762/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.0703 - accuracy: 0.9647\n",
            "Epoch 763/800\n",
            "59/59 [==============================] - 5s 88ms/step - loss: 0.0703 - accuracy: 0.9652\n",
            "Epoch 764/800\n",
            "59/59 [==============================] - 7s 112ms/step - loss: 0.0737 - accuracy: 0.9647\n",
            "Epoch 765/800\n",
            "59/59 [==============================] - 4s 75ms/step - loss: 0.0670 - accuracy: 0.9684\n",
            "Epoch 766/800\n",
            "59/59 [==============================] - 6s 101ms/step - loss: 0.0758 - accuracy: 0.9615\n",
            "Epoch 767/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0736 - accuracy: 0.9673\n",
            "Epoch 768/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.0747 - accuracy: 0.9641\n",
            "Epoch 769/800\n",
            "59/59 [==============================] - 6s 103ms/step - loss: 0.0729 - accuracy: 0.9652\n",
            "Epoch 770/800\n",
            "59/59 [==============================] - 4s 73ms/step - loss: 0.0723 - accuracy: 0.9652\n",
            "Epoch 771/800\n",
            "59/59 [==============================] - 5s 83ms/step - loss: 0.0731 - accuracy: 0.9636\n",
            "Epoch 772/800\n",
            "59/59 [==============================] - 5s 91ms/step - loss: 0.0693 - accuracy: 0.9706\n",
            "Epoch 773/800\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 0.0686 - accuracy: 0.9690\n",
            "Epoch 774/800\n",
            "59/59 [==============================] - 6s 98ms/step - loss: 0.0716 - accuracy: 0.9636\n",
            "Epoch 775/800\n",
            "59/59 [==============================] - 5s 82ms/step - loss: 0.0722 - accuracy: 0.9657\n",
            "Epoch 776/800\n",
            "59/59 [==============================] - 4s 76ms/step - loss: 0.0712 - accuracy: 0.9668\n",
            "Epoch 777/800\n",
            "59/59 [==============================] - 6s 103ms/step - loss: 0.0730 - accuracy: 0.9615\n",
            "Epoch 778/800\n",
            "59/59 [==============================] - 4s 74ms/step - loss: 0.0674 - accuracy: 0.9652\n",
            "Epoch 779/800\n",
            "59/59 [==============================] - 4s 72ms/step - loss: 0.0727 - accuracy: 0.9625\n",
            "Epoch 780/800\n",
            "59/59 [==============================] - 6s 100ms/step - loss: 0.0687 - accuracy: 0.9695\n",
            "Epoch 781/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0698 - accuracy: 0.9668\n",
            "Epoch 782/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0672 - accuracy: 0.9673\n",
            "Epoch 783/800\n",
            "59/59 [==============================] - 6s 94ms/step - loss: 0.0699 - accuracy: 0.9657\n",
            "Epoch 784/800\n",
            "59/59 [==============================] - 4s 69ms/step - loss: 0.0688 - accuracy: 0.9641\n",
            "Epoch 785/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0701 - accuracy: 0.9641\n",
            "Epoch 786/800\n",
            "59/59 [==============================] - 8s 128ms/step - loss: 0.0701 - accuracy: 0.9631\n",
            "Epoch 787/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0683 - accuracy: 0.9684\n",
            "Epoch 788/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 0.0702 - accuracy: 0.9663\n",
            "Epoch 789/800\n",
            "59/59 [==============================] - 5s 80ms/step - loss: 0.0795 - accuracy: 0.9599\n",
            "Epoch 790/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0712 - accuracy: 0.9636\n",
            "Epoch 791/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 0.0720 - accuracy: 0.9641\n",
            "Epoch 792/800\n",
            "59/59 [==============================] - 5s 81ms/step - loss: 0.0707 - accuracy: 0.9647\n",
            "Epoch 793/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0704 - accuracy: 0.9668\n",
            "Epoch 794/800\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 0.0735 - accuracy: 0.9641\n",
            "Epoch 795/800\n",
            "59/59 [==============================] - 5s 77ms/step - loss: 0.0743 - accuracy: 0.9636\n",
            "Epoch 796/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0699 - accuracy: 0.9641\n",
            "Epoch 797/800\n",
            "59/59 [==============================] - 6s 98ms/step - loss: 0.0722 - accuracy: 0.9673\n",
            "Epoch 798/800\n",
            "59/59 [==============================] - 4s 70ms/step - loss: 0.0698 - accuracy: 0.9647\n",
            "Epoch 799/800\n",
            "59/59 [==============================] - 4s 71ms/step - loss: 0.0671 - accuracy: 0.9690\n",
            "Epoch 800/800\n",
            "59/59 [==============================] - 6s 99ms/step - loss: 0.0701 - accuracy: 0.9647\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x797af90c4e50>"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# entrenaremos nuestro modelo con 100 épocas\n",
        "model.fit(predictors, label, epochs=800, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZRehP7pp5KBm"
      },
      "outputs": [],
      "source": [
        "# crear función\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len, tk):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tk.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        #predicted = model.predict_classes(token_list, verbose=0)\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        predicted = np.argmax(predicted, axis=1)\n",
        "        output_word = \"\"\n",
        "        for word,index in tk.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \"+output_word\n",
        "    return seed_text.title()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WpUVG1Do5Vuc",
        "outputId": "350e81ab-963a-40cc-b6ad-0684683b5f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mujeres Cual Significa “Amor\n",
            "Mujeres Cual Significa “Amor A\n",
            "Mujeres Cual Significa “Amor A La\n",
            "Mujeres Cual Significa “Amor A La Sabiduría”\n"
          ]
        }
      ],
      "source": [
        "print( generate_text(\"mujeres\" ,  3 , model,max_tam ,tokenizador))\n",
        "print( generate_text(\"mujeres\" ,  4 , model,max_tam ,tokenizador))\n",
        "print( generate_text(\"mujeres\" ,  5 , model,max_tam ,tokenizador))\n",
        "print( generate_text(\"mujeres\" ,  6 , model,max_tam ,tokenizador))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}